{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed5aad45",
   "metadata": {},
   "source": [
    "# Telecom Churn Case Study - Kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d49ca6d",
   "metadata": {},
   "source": [
    "`References`\n",
    "\n",
    "\n",
    "- https://medium.com/analytics-vidhya/telecom-churn-prediction-9ce72c24e961\n",
    "- https://github.com/akashkriplani/telecom-churn-case-study/tree/main\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a73769",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "This is a student exercise using telecom churn data. The goal is to build a model to predict customer churn probability. The data is provided in two parts: one dataset with known churn outcomes (`train.csv`) and another without churn outcomes (`test.csv`).\n",
    "\n",
    "### Exercise Overview:\n",
    "\n",
    "1. **80% weightage:** Focus on building the best model for the business caseâ€”predicting churn probability based on multiple KPIs.\n",
    "2. **20% weightage:** A submission to a Kaggle competition, where the model is evaluated on unseen data (`test.csv`).\n",
    "\n",
    "The data contains KPIs measured across months, and our EDA will explore how these KPIs evolve over time. We will also test different models to compare their performance and choose the best one for Kaggle submission.\n",
    "\n",
    "The model with the highest accuracy will be selected for the Kaggle submission, even if it is not the best across all evaluation metrics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d42f5a",
   "metadata": {},
   "source": [
    "# Module imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "97992874",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd ; pd.set_option('display.max_rows', 500)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "f0c2102d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab443b51",
   "metadata": {},
   "source": [
    "# Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "baf12fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Any\n",
    "from sklearn.experimental import enable_iterative_imputer  # Required to use IterativeImputer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.impute import IterativeImputer, KNNImputer\n",
    "\n",
    "def detailed_summary(data: pd.DataFrame, display_top_n=3) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Provides detailed summary for both categorical and numerical variables.\n",
    "\n",
    "    Data    : Pandas dataframe\n",
    "    display_top_n : Number of top frequent values to display for categorical variables.\n",
    "    \n",
    "    Returns :\n",
    "        Pandas dataframe with descriptive summary for both categorical and numerical columns.\n",
    "    \"\"\"\n",
    "    # Separate numerical and categorical columns\n",
    "    categorical_columns = data.select_dtypes(exclude=['int64', 'float64']).columns\n",
    "    numerical_columns = data.select_dtypes(include=['int64', 'float64']).columns\n",
    "    \n",
    "    numerical_summary = pd.DataFrame()\n",
    "    categorical_summary = pd.DataFrame()\n",
    "\n",
    "    if len(numerical_columns) > 0:\n",
    "        # Summary for numerical columns\n",
    "        numerical_summary = data[numerical_columns].describe().T\n",
    "        numerical_summary['count'] = data[numerical_columns].count()\n",
    "        numerical_summary['dtype'] = data[numerical_columns].dtypes\n",
    "        numerical_summary['missing_values %'] = round(data[numerical_columns].isna().sum() / len(data) * 100, 2)\n",
    "        # numerical_summary['skew'] = data[numerical_columns].skew()\n",
    "        # numerical_summary['kurtosis'] = data[numerical_columns].kurtosis()\n",
    "        numerical_summary['unique_values'] = data[numerical_columns].nunique(dropna=True)\n",
    "        numerical_summary['single_unique_value'] = data[numerical_columns].apply(lambda x: 'Yes' if x.nunique(dropna=True) == 1 else 'No')\n",
    "\n",
    "    if len(categorical_columns) > 0:\n",
    "        # Summary for categorical columns\n",
    "        categorical_summary = pd.DataFrame(index=categorical_columns)\n",
    "        categorical_summary['count'] = data[categorical_columns].count()\n",
    "        categorical_summary['dtype'] = data[categorical_columns].dtypes\n",
    "        categorical_summary['missing_values %'] = round(data[categorical_columns].isna().sum() / len(data) * 100, 2)\n",
    "        categorical_summary['unique_values'] = data[categorical_columns].nunique()\n",
    "        categorical_summary['most_frequent'] = data[categorical_columns].apply(lambda x: x.mode().iloc[0] if not x.mode().empty else None)\n",
    "        categorical_summary['frequency'] = data[categorical_columns].apply(lambda x: x.value_counts().iloc[0] if not x.value_counts().empty else None)\n",
    "        # categorical_summary['top_n_frequent'] = data[categorical_columns].apply(lambda x: dict(x.value_counts().head(display_top_n)))\n",
    "        # categorical_summary['single_unique_value'] = data[categorical_columns].apply(lambda x: 'Yes' if x.nunique() == 1 else 'No')\n",
    "\n",
    "    # Combine both summaries\n",
    "    return pd.concat([numerical_summary, categorical_summary], axis=0)\n",
    "\n",
    "def find_columns_by_string(data : pd.DataFrame, search_string : str):\n",
    "    \"\"\"\n",
    "    find columns which has given search string\n",
    "    \"\"\"\n",
    "    return [col for col in data.columns if search_string.lower() in col.lower()]\n",
    "\n",
    "def find_columns_by_regex(data, regex_pattern):\n",
    "    \"\"\"\n",
    "    find columns which has search string matching regex pattern\n",
    "    \"\"\"\n",
    "    return [col for col in data.columns if re.search(regex_pattern, col, re.IGNORECASE)]\n",
    "\n",
    "def handle_missing_values(data: pd.DataFrame, strategy=\"mean\", impute_categorical=False, knn_neighbors=5) -> Tuple[Any, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    TODO: Function need work. handling of mix dtypes is not done correctly.\n",
    "\n",
    "    Handles missing values in the dataframe based on the provided strategy.\n",
    "    \n",
    "    strategy : str, default \"mean\"\n",
    "        Imputation strategy for numerical columns:\n",
    "        - \"mean\": replaces missing values with mean\n",
    "        - \"median\": replaces missing values with median\n",
    "        - \"most_frequent\": replaces missing values with the most frequent value\n",
    "        - \"knn\": uses KNN imputer\n",
    "        - \"iterative\": uses Iterative imputer (more advanced)\n",
    "    \n",
    "    impute_categorical : bool, default True\n",
    "        Whether to impute categorical variables with the most frequent value.\n",
    "    \n",
    "    knn_neighbors : int, default 5\n",
    "        Number of neighbors to use for KNN imputation.\n",
    "    \n",
    "    Returns:\n",
    "        imputer --> useful for imputing test sets or review model parameters.\n",
    "        DataFrame with missing values handled.\n",
    "    \"\"\"\n",
    "    \n",
    "    data_cleaned = data.copy()\n",
    "    \n",
    "    # Separate numerical and categorical columns\n",
    "    numerical_cols = data_cleaned.select_dtypes(include=['float64', 'int64']).columns\n",
    "    categorical_cols = data_cleaned.select_dtypes(exclude=['float64', 'int64']).columns\n",
    "    \n",
    "    if len(numerical_cols) > 0:\n",
    "        # Handle missing values for numerical columns\n",
    "        if strategy in [\"mean\", \"median\", \"most_frequent\"]:\n",
    "            imputer = SimpleImputer(strategy=strategy)\n",
    "            data_cleaned[numerical_cols] = imputer.fit_transform(data_cleaned[numerical_cols])\n",
    "        \n",
    "        elif strategy == \"knn\":\n",
    "            imputer = KNNImputer(n_neighbors=knn_neighbors)\n",
    "            data_cleaned[numerical_cols] = imputer.fit_transform(data_cleaned[numerical_cols])\n",
    "        \n",
    "        elif strategy == \"iterative\":\n",
    "            imputer = IterativeImputer()\n",
    "            data_cleaned[numerical_cols] = imputer.fit_transform(data_cleaned[numerical_cols])\n",
    "    \n",
    "    # Handle missing values for categorical columns (if applicable)\n",
    "    if len(categorical_cols) > 0 and impute_categorical:\n",
    "        imputer = SimpleImputer(strategy=\"most_frequent\")\n",
    "        data_cleaned[categorical_cols] = imputer.fit_transform(data_cleaned[categorical_cols])\n",
    "    \n",
    "    return imputer, data_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70483f4",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73bf6f4",
   "metadata": {},
   "source": [
    "### Links to data on google drive \n",
    "\n",
    "- https://drive.google.com/uc?id=1BTXvDT0JQK_bNXEMBJCSl4AWgDlBHMVv - train.csv\n",
    "- https://drive.google.com/uc?id=1NeIn5Y199H1WNYxZbryUwrKdxHw3tjHz - test.csv\n",
    "- https://drive.google.com/uc?id=1cN3iMwxib1a_8POB3DZDf9g9_kQJ-onV - sample.csv\n",
    "- https://drive.google.com/uc?id=11YclgZdYOuJYptrNutqCOoLgf4bz9xL8 - dictionary.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "760c09c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_csv(\"data/train.csv\")\n",
    "# unseen = pd.read_csv(\"data/test.csv\")\n",
    "# kaggle_submisson_sample = pd.read_csv(\"data/sample.csv\")\n",
    "# data_dictionary = pd.read_csv(\"data/data_dictionary.csv\")\n",
    "\n",
    "# Load data from google drive.\n",
    "data = pd.read_csv(\"https://drive.google.com/uc?id=1BTXvDT0JQK_bNXEMBJCSl4AWgDlBHMVv\")\n",
    "unseen = pd.read_csv(\"https://drive.google.com/uc?id=1NeIn5Y199H1WNYxZbryUwrKdxHw3tjHz\")\n",
    "kaggle_submisson_sample = pd.read_csv(\"https://drive.google.com/uc?id=1cN3iMwxib1a_8POB3DZDf9g9_kQJ-onV\")\n",
    "data_dictionary = pd.read_csv(\"https://drive.google.com/uc?id=11YclgZdYOuJYptrNutqCOoLgf4bz9xL8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"train.csv\", data.shape)\n",
    "print(\"test.csv\", unseen.shape)\n",
    "print(\"sample.csv\", kaggle_submisson_sample.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0948952d",
   "metadata": {},
   "source": [
    "`data_dictionary.csv`\n",
    "\n",
    "|Acronyms|Description|\n",
    "|---|---|\n",
    "|CIRCLE_ID|Telecom circle area to which the customer belongs to|\n",
    "|LOC|Local calls  within same telecom circle|\n",
    "|STD|STD calls  outside the calling circle|\n",
    "|IC|Incoming calls|\n",
    "|OG|Outgoing calls|\n",
    "|T2T|Operator T to T ie within same operator mobile to mobile|\n",
    "|T2M    |Operator T to other operator mobile|\n",
    "|T2O    |Operator T to other operator fixed line|\n",
    "|T2F    |Operator T to fixed lines of T|\n",
    "|T2C    |Operator T to its own call center|\n",
    "|ARPU    |Average revenue per user|\n",
    "|MOU    |Minutes of usage  voice calls|\n",
    "|AON    |Age on network  number of days the customer is using the operator T network|\n",
    "|ONNET   |All kind of calls within the same operator network|\n",
    "|OFFNET    |All kind of calls outside the operator T network|\n",
    "|ROAM|Indicates that customer is in roaming zone during the call|\n",
    "|SPL   |Special calls|\n",
    "|ISD    |ISD calls|\n",
    "|RECH    |Recharge|\n",
    "|NUM    |Number|\n",
    "|AMT    |Amount in local currency|\n",
    "|MAX    |Maximum|\n",
    "|DATA    |Mobile internet|\n",
    "|3G    |G network|\n",
    "|AV    |Average|\n",
    "|VOL    |Mobile internet usage volume in MB|\n",
    "|2G    |G network|\n",
    "|PCK    |Prepaid service schemes called  PACKS|\n",
    "|NIGHT    |Scheme to use during specific night hours only|\n",
    "|MONTHLY    |Service schemes with validity equivalent to a month|\n",
    "|SACHET   |Service schemes with validity smaller than a month|\n",
    "|*.6    |KPI for the month of June|\n",
    "|*.7    |KPI for the month of July|\n",
    "|*.8    |KPI for the month of August|\n",
    "|FB_USER|Service scheme to avail services of Facebook and similar social networking sites|\n",
    "|VBC    |Volume based cost  when no specific scheme is not purchased and paid as per usage|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f5d85c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0e9f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "detailed_summary(data).sort_values(by=['unique_values'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7dec929",
   "metadata": {},
   "source": [
    "## Initial review of data summary\n",
    "1. Column `circle_id` can go as it only has one unique value across all users.\n",
    "2. We have also have columns last_date_of_month_6,  last_date_of_month_7 and last_date_of_month_8 which can go as well as these are just date for end of the month.\n",
    "3. We have columns fb_user_* with binary value and most likley defining user or non user of fb pack. These columns have high missig value of ~74%. Same with columns night_pck_user_*\n",
    "4. Columns 'std_ic_t2o_mou_6', 'std_ic_t2o_mou_7' and 'std_ic_t2o_mou_8' have all zeros and few missing.\n",
    "5. Columns 'loc_og_t2o_mou', 'std_og_t2o_mou' and 'loc_ic_t2o_mou' have same story of all zeroes but no missing.  \n",
    "6. Columns 'std_og_t2c_mou_6', 'std_og_t2c_mou_7' and 'std_og_t2c_mou_8' have all zeros and few missing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd96a0e9",
   "metadata": {},
   "source": [
    "# Task at hand\n",
    "\n",
    "`Task at hand is to build model to predict churn for high value customers.` We will split data by high value customer. This should help lightern the load as we deal with less records in further steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6a6297",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcb7c42",
   "metadata": {},
   "source": [
    "## Dropping columns based on initial review\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c34fa01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "columns_to_drop_initial_review = ['circle_id', \n",
    "                                  'loc_og_t2o_mou', \n",
    "                                  'std_og_t2o_mou', \n",
    "                                  'loc_ic_t2o_mou', \n",
    "                                  'std_ic_t2o_mou_6', 'std_ic_t2o_mou_7', 'std_ic_t2o_mou_8',\n",
    "                                  'std_og_t2c_mou_6', 'std_og_t2c_mou_7', 'std_og_t2c_mou_8',\n",
    "                                  'last_date_of_month_6', 'last_date_of_month_7', 'last_date_of_month_8'\n",
    "] \n",
    "\n",
    "data = data.drop(columns=columns_to_drop_initial_review)\n",
    "unseen = unseen.drop(columns=columns_to_drop_initial_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e850e811",
   "metadata": {},
   "source": [
    "We have removed all columns with unique values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9ae962",
   "metadata": {},
   "source": [
    "## Fix data type for categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc12ef4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(data.nunique()).sort_values(by=[0]).head(n=10) # Checking if we have columns with unique values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c68cd96",
   "metadata": {},
   "source": [
    "Let's put fb_user_* and night_pck_user_* into a bucket as these are categorical columns with True and False values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8d5f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_categorical = find_columns_by_regex(data=data, regex_pattern=r'.*fb_user.*|.*night_pck_user.*')\n",
    "binary_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "67489b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[binary_categorical] = data[binary_categorical].astype('category')\n",
    "unseen[binary_categorical] = unseen[binary_categorical].astype('category')\n",
    "data[['churn_probability']] = data[['churn_probability']].astype('category') # Reminding ourselves churn_probability is not included in the unseen data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d82e844",
   "metadata": {},
   "source": [
    "## Date columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fedecd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_columns = [v for v in data.columns if v.startswith('date_')]\n",
    "date_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd2d3a1",
   "metadata": {},
   "source": [
    "We have determined that the dates provided represent the last recharge within each month. However, we do not know whether the absence of a recharge in a given month indicates that the customer did not recharge at all or had opted for a longer validity in the previous month.\n",
    "\n",
    "Due to the lack of additional information, we will drop these columns as they cannot be reliably used in our analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ee773164",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(columns=date_columns)\n",
    "unseen = unseen.drop(columns=date_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56849369",
   "metadata": {},
   "source": [
    "## Get columns with high missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135cdf86",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_with_70perct_missing = list(data.columns[data.isnull().mean() > 0.50])\n",
    "columns_with_70perct_missing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163bb1e2",
   "metadata": {},
   "source": [
    "We have determined that these columns do not actually have missing data; instead, the absence of data carries meaning. We will handle each set of columns accordingly based on their specific context.\n",
    "\n",
    "- Variables like `total_rech_num_6` can be filled with `0` to indicate zero recharges.\n",
    "- Columns such as `night_pck_user_*` and `fb_user_*` are categorical, and missing data indicates the absence of a subscription to the service. We will mark these missing values as `-1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4adf42",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_zero_fill = find_columns_by_regex(data=data,\n",
    "                                             regex_pattern=r'_rech_|arpu_3g|arpu_2g')\n",
    "columns_to_zero_fill\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823267b8",
   "metadata": {},
   "source": [
    "## Columns with high missing values to fill with zero\n",
    "\n",
    "```python \n",
    "[\n",
    "    'total_rech_num_6',\n",
    "    'total_rech_num_7',\n",
    "    'total_rech_num_8',\n",
    "    'total_rech_amt_6',\n",
    "    'total_rech_amt_7',\n",
    "    'total_rech_amt_8',\n",
    "    'max_rech_amt_6',\n",
    "    'max_rech_amt_7',\n",
    "    'max_rech_amt_8',\n",
    "    'total_rech_data_6',\n",
    "    'total_rech_data_7',\n",
    "    'total_rech_data_8',\n",
    "    'max_rech_data_6',\n",
    "    'max_rech_data_7',\n",
    "    'max_rech_data_8',\n",
    "    'count_rech_2g_6',\n",
    "    'count_rech_2g_7',\n",
    "    'count_rech_2g_8',\n",
    "    'count_rech_3g_6',\n",
    "    'count_rech_3g_7',\n",
    "    'count_rech_3g_8',\n",
    "    'av_rech_amt_data_6',\n",
    "    'av_rech_amt_data_7',\n",
    "    'av_rech_amt_data_8',\n",
    "    'arpu_3g_6',\n",
    "    'arpu_3g_7',\n",
    "    'arpu_3g_8',\n",
    "    'arpu_2g_6',\n",
    "    'arpu_2g_7',\n",
    "    'arpu_2g_8'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c733525c",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_zero_fill = [\n",
    "    'total_rech_num_6',\n",
    "    'total_rech_num_7',\n",
    "    'total_rech_num_8',\n",
    "    'total_rech_amt_6',\n",
    "    'total_rech_amt_7',\n",
    "    'total_rech_amt_8',\n",
    "    'max_rech_amt_6',\n",
    "    'max_rech_amt_7',\n",
    "    'max_rech_amt_8',\n",
    "    'total_rech_data_6',\n",
    "    'total_rech_data_7',\n",
    "    'total_rech_data_8',\n",
    "    'max_rech_data_6',\n",
    "    'max_rech_data_7',\n",
    "    'max_rech_data_8',\n",
    "    'count_rech_2g_6',\n",
    "    'count_rech_2g_7',\n",
    "    'count_rech_2g_8',\n",
    "    'count_rech_3g_6',\n",
    "    'count_rech_3g_7',\n",
    "    'count_rech_3g_8',\n",
    "    'av_rech_amt_data_6',\n",
    "    'av_rech_amt_data_7',\n",
    "    'av_rech_amt_data_8',\n",
    "    'arpu_3g_6',\n",
    "    'arpu_3g_7',\n",
    "    'arpu_3g_8',\n",
    "    'arpu_2g_6',\n",
    "    'arpu_2g_7',\n",
    "    'arpu_2g_8'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1d9df67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[columns_to_zero_fill] = data[columns_to_zero_fill].fillna(0)\n",
    "unseen[columns_to_zero_fill] = unseen[columns_to_zero_fill].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f513a84c",
   "metadata": {},
   "source": [
    "## Categorical variables to fill with -1 to peserve meaning of the existing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "daf3a78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add -1 as a new category to the categorical columns\n",
    "data[binary_categorical] = data[binary_categorical].apply(lambda col: col.cat.add_categories([-1]))\n",
    "unseen[binary_categorical] = unseen[binary_categorical].apply(lambda col: col.cat.add_categories([-1]))\n",
    "\n",
    "# Now, fill missing values with -1\n",
    "data[binary_categorical] = data[binary_categorical].fillna(-1)\n",
    "unseen[binary_categorical] = unseen[binary_categorical].fillna(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e7f755",
   "metadata": {},
   "source": [
    "## Columns with low missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72376126",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_with_few_missing = list(data.columns[(data.isnull().mean() < 0.50) & (data.isnull().mean() > 0)])\n",
    "len(columns_with_few_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e504cde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[columns_with_few_missing].head(n=5).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bc144f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple_imputer, imputed_data = handle_missing_values(\n",
    "#     data=data[columns_with_few_missing],\n",
    "#     strategy='knn',\n",
    "#     impute_categorical=False\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14457fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "KNNImputer, imputed_data = handle_missing_values(\n",
    "    data=data[columns_with_few_missing],\n",
    "    strategy='knn',\n",
    "    impute_categorical=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed939d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "detailed_summary(data[columns_with_few_missing]).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d868050c",
   "metadata": {},
   "outputs": [],
   "source": [
    "detailed_summary(imputed_data).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c938aee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[columns_with_few_missing] = imputed_data                                               # Train data updated with imputed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d56ea5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "unseen[columns_with_few_missing] = KNNImputer.transform(unseen[columns_with_few_missing] )  # Unseen data transformed with imputer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ecc93b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert ([] == list(data.columns[(data.isnull().mean() > 0)]))       # this list should be empty indicating there are no columns with missing values at this stage.\n",
    "assert ([] == list(unseen.columns[(unseen.isnull().mean() > 0)]))   # this list should be empty indicating there are no columns with missing values at this stage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef49d4b0",
   "metadata": {},
   "source": [
    "# High Value customers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69c7783",
   "metadata": {},
   "source": [
    "### High-Value Customer Definition\n",
    "\n",
    "`reference` : https://github.com/akashkriplani/telecom-churn-case-study/tree/main\n",
    "\n",
    "To identify high-value customers, we look at their total recharge behavior, focusing on both regular and data recharges over two months (months 6 and 7):\n",
    "\n",
    "- **Recharge Calculation**: We consider both the number of data recharges (`total_rech_data_6`, `total_rech_data_7`) and the average recharge amounts for these recharges (`av_rech_amt_data_6`, `av_rech_amt_data_7`). These are combined with regular recharge amounts (`total_rech_amt_6`, `total_rech_amt_7`) to get the total recharge value for each month.\n",
    "\n",
    "- **Average Recharge Amount**: The average of the total recharge amounts from months 6 and 7 is then calculated for each customer, giving an indicator of their overall recharge behavior.\n",
    "\n",
    "- **High-Value Customer Flag**: A cutoff is set at the 70th percentile of these average recharge values. Customers exceeding this threshold are flagged as high-value, indicating they are among the top spenders in terms of recharges.\n",
    "\n",
    "This approach ensures that high-value customers are identified based on their significant recharge activity across both months, excluding churn month of August."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7dd33fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = 0.7\n",
    "\n",
    "# Calculate total recharge amounts for months 6 and 7 (data + regular recharges)\n",
    "data['rech_data_amt_6'] = data['total_rech_data_6'] * data['av_rech_amt_data_6']\n",
    "data['rech_data_amt_7'] = data['total_rech_data_7'] * data['av_rech_amt_data_7']\n",
    "\n",
    "data['total_amt_6'] = data['total_rech_amt_6'] + data['rech_data_amt_6']\n",
    "data['total_amt_7'] = data['total_rech_amt_7'] + data['rech_data_amt_7']\n",
    "\n",
    "# Compute the average total amount for months 6 and 7\n",
    "data['avg_rech_amt_6_7'] = data[['total_amt_6', 'total_amt_7']].mean(axis=1)\n",
    "\n",
    "# Define the cutoff for high-value customers\n",
    "high_value_cutoff = data['avg_rech_amt_6_7'].quantile(cutoff)\n",
    "\n",
    "# Create a flag for high-value customers\n",
    "high_value_customer_train = data['avg_rech_amt_6_7'] > high_value_cutoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270b4928",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(high_value_customer_train).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510274bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate total recharge amounts for months 6 and 7 (unseen + regular recharges)\n",
    "unseen['rech_data_amt_6'] = unseen['total_rech_data_6'] * unseen['av_rech_amt_data_6']\n",
    "unseen['rech_data_amt_7'] = unseen['total_rech_data_7'] * unseen['av_rech_amt_data_7']\n",
    "\n",
    "unseen['total_amt_6'] = unseen['total_rech_amt_6'] + unseen['rech_data_amt_6']\n",
    "unseen['total_amt_7'] = unseen['total_rech_amt_7'] + unseen['rech_data_amt_7']\n",
    "\n",
    "# Compute the average total amount for months 6 and 7\n",
    "unseen['avg_rech_amt_6_7'] = unseen[['total_amt_6', 'total_amt_7']].mean(axis=1)\n",
    "\n",
    "# Define the cutoff for high-value customers\n",
    "# Using cut off as per train data\n",
    "\n",
    "# Create a flag for high-value customers\n",
    "high_value_customer_unseen = unseen['avg_rech_amt_6_7'] > high_value_cutoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a22aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(high_value_customer_train).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fa35a6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_copy = data.copy(deep=True)\n",
    "data = data[high_value_customer_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e68af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(data.shape[1] == unseen.shape[1]+1) # Unseen has one less column\n",
    "data.shape, unseen.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d83824",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9cb1ac",
   "metadata": {},
   "source": [
    "## Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4049b2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are not only exporting correlation scores to CSV but also ensuring each pair only appears once, reducing overload of information. \n",
    "\n",
    "# Create a temporary copy of the data without the 'id' column\n",
    "data_temp = data.drop(columns=['id'])\n",
    "\n",
    "# Divide the data by churn probability\n",
    "data_churn_0 = data_temp[data_temp['churn_probability'] == 0]\n",
    "data_churn_1 = data_temp[data_temp['churn_probability'] == 1]\n",
    "\n",
    "# Calculate the correlation matrix for each group and the overall correlation matrix\n",
    "correlation_matrix_total = data_temp.corr().round(4)\n",
    "correlation_matrix_0 = data_churn_0.corr().round(4)\n",
    "correlation_matrix_1 = data_churn_1.corr().round(4)\n",
    "\n",
    "# Get the upper triangle of the correlation matrix for each group and the total\n",
    "upper_triangle_total = correlation_matrix_total.where(\n",
    "    np.triu(np.ones(correlation_matrix_total.shape), k=1).astype(bool)\n",
    ")\n",
    "upper_triangle_0 = correlation_matrix_0.where(\n",
    "    np.triu(np.ones(correlation_matrix_0.shape), k=1).astype(bool)\n",
    ")\n",
    "upper_triangle_1 = correlation_matrix_1.where(\n",
    "    np.triu(np.ones(correlation_matrix_1.shape), k=1).astype(bool)\n",
    ")\n",
    "\n",
    "# Unstack the matrices and drop NaN values\n",
    "correlation_pairs_total = upper_triangle_total.unstack().dropna()\n",
    "correlation_pairs_0 = upper_triangle_0.unstack().dropna()\n",
    "correlation_pairs_1 = upper_triangle_1.unstack().dropna()\n",
    "\n",
    "# Convert the correlation pairs to dataframes\n",
    "correlation_pairs_df_total = correlation_pairs_total.reset_index()\n",
    "correlation_pairs_df_total.columns = ['Variable 1', 'Variable 2', 'Correlation (Total)']\n",
    "\n",
    "correlation_pairs_df_0 = correlation_pairs_0.reset_index()\n",
    "correlation_pairs_df_0.columns = ['Variable 1', 'Variable 2', 'Correlation (Churn=0)']\n",
    "\n",
    "correlation_pairs_df_1 = correlation_pairs_1.reset_index()\n",
    "correlation_pairs_df_1.columns = ['Variable 1', 'Variable 2', 'Correlation (Churn=1)']\n",
    "\n",
    "# Merge the correlation dataframes\n",
    "correlation_pairs_df = correlation_pairs_df_total.merge(correlation_pairs_df_0, on=['Variable 1', 'Variable 2'], how='left')\n",
    "correlation_pairs_df = correlation_pairs_df.merge(correlation_pairs_df_1, on=['Variable 1', 'Variable 2'], how='left')\n",
    "correlation_pairs_df['Correlation (Total) (ABS)'] = correlation_pairs_df['Correlation (Total)'].abs()\n",
    "correlation_pairs_df['Correlation (Churn=0) (ABS)'] = correlation_pairs_df['Correlation (Churn=0)'].abs()\n",
    "correlation_pairs_df['Correlation (Churn=1) (ABS)'] = correlation_pairs_df['Correlation (Churn=1)'].abs()\n",
    "\n",
    "# Save the merged correlation pairs to a CSV file\n",
    "correlation_pairs_df.to_csv('correlation_pairs_combined.csv', index=False)\n",
    "\n",
    "print(\"Combined correlation pairs saved to 'correlation_pairs_combined.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22a733c",
   "metadata": {},
   "source": [
    "1. Revenue v/s recharge\n",
    "- Correlation of revenue with respective month indicate recharge for the month is directly reflected towards revenue for the month\n",
    "- Low correlation between revenue and recharge amounged churned users may also indicate users tend to spend less as they churn\n",
    "- Number of recharge follow similat trend\n",
    "\n",
    "| Variable 1 | Variable 2 | Correlation (Total) | Correlation (Churn=0) | Correlation (Churn=1) |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| total_rech_amt_6 | arpu_6 | 0.9528 | 0.9536 | 0.9477 |\n",
    "| total_rech_amt_6 | arpu_7 | 0.7511 | 0.7880 | 0.4823 |\n",
    "| total_rech_amt_6 | arpu_8 | 0.6390 | 0.6936 | 0.2420 |\n",
    "| total_rech_amt_7 | arpu_6 | 0.7260 | 0.7705 | 0.4173 |\n",
    "| total_rech_amt_7 | arpu_7 | 0.9553 | 0.9560 | 0.9461 |\n",
    "| total_rech_amt_7 | arpu_8 | 0.7788 | 0.8003 | 0.5374 |\n",
    "| total_rech_amt_8 | arpu_6 | 0.6268 | 0.6871 | 0.1989 |\n",
    "| total_rech_amt_8 | arpu_7 | 0.7672 | 0.7913 | 0.4927 |\n",
    "| total_rech_amt_8 | arpu_8 | 0.9583 | 0.9582 | 0.9405 |\n",
    "\n",
    "<br />\n",
    "\n",
    "| Variable 1 | Variable 2 | Correlation (Total) | Correlation (Churn=0) | Correlation (Churn=1) |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| total_rech_num_7 | total_rech_num_6 | 0.6703 | 0.7002 | 0.4854 |\n",
    "| total_rech_num_8 | total_rech_num_6 | 0.5167 | 0.5649 | 0.2084 |\n",
    "| total_rech_num_8 | total_rech_num_7 | 0.7239 | 0.7418 | 0.5518 |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f7bc60",
   "metadata": {},
   "source": [
    "2. Outgoin ISD calls\n",
    "\n",
    "- Users likey start making less call as the are about to churn as shown in reduced correlation in churn v/s previous month \n",
    "- User which do not churn continue to likely make similar amount of calls\n",
    "\n",
    "| Variable 1 | Variable 2 | Correlation (Total) | Correlation (Churn=0) | Correlation (Churn=1) |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| total_og_mou_8 | total_og_mou_7 | 0.7693 | 0.7928 | 0.5873 |\n",
    "| total_og_mou_7 | total_og_mou_6 | 0.7245 | 0.7464 | 0.6102 |\n",
    "| total_og_mou_8 | total_og_mou_6 | 0.5659 | 0.6157 | 0.2840 |\n",
    "\n",
    "<br />\n",
    "\n",
    "| Variable 1 | Variable 2 | Correlation (Total) | Correlation (Churn=0) | Correlation (Churn=1) |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| isd_og_mou_8 | isd_og_mou_7 | 0.9470 | 0.9582 | 0.5492 |\n",
    "| isd_og_mou_7 | isd_og_mou_6 | 0.9340 | 0.9588 | 0.4444 |\n",
    "| isd_og_mou_8 | isd_og_mou_6 | 0.9241 | 0.9526 | 0.2640 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46895d9",
   "metadata": {},
   "source": [
    "3. Churn v/s age of network\n",
    "- Very low negative correlation indicating as age increases churn probability reduces but is not guaranteed\n",
    "- This correlation is likely affected by class imbalance as well and I would other expect bit higher negative correlation\n",
    "\n",
    "| Variable 1 | Variable 2 | Correlation (Total) |\n",
    "| --- | --- | --- |\n",
    "| churn_probability | aon | -0.1352 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e925e4f9",
   "metadata": {},
   "source": [
    "4. Within operator network and outside operator network\n",
    "- Similar trend of low correlation between months which are further apart, indicating call volume goes down as people churn.\n",
    "\n",
    "| Variable 1 | Variable 2 | Correlation (Total) | Correlation (Churn=0) | Correlation (Churn=1) |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| onnet_mou_8 | onnet_mou_7 | 0.7875 | 0.8081 | 0.6340 |\n",
    "| onnet_mou_7 | onnet_mou_6 | 0.7339 | 0.7510 | 0.6528 |\n",
    "| onnet_mou_8 | onnet_mou_6 | 0.5978 | 0.6421 | 0.3567 |\n",
    "\n",
    "<br />\n",
    "\n",
    "| Variable 1 | Variable 2 | Correlation (Total) | Correlation (Churn=0) | Correlation (Churn=1) |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| offnet_mou_8 | offnet_mou_7 | 0.7557 | 0.7796 | 0.5331 |\n",
    "| offnet_mou_7 | offnet_mou_6 | 0.7405 | 0.7597 | 0.6321 |\n",
    "| offnet_mou_8 | offnet_mou_6 | 0.5734 | 0.6178 | 0.2713 |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d03636",
   "metadata": {},
   "source": [
    "## Change in monthly KPI churn vs non-churn\n",
    "Let's explore these monthly KPI relationship little bit more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "07fe171a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_monthly_column_groups(data: pd.DataFrame) -> list[tuple[str, str, str]]:\n",
    "    \"\"\"\n",
    "    Find columns with names ending in _6, _7, and _8 and return them as a list of tuples.\n",
    "\n",
    "    Parameters:\n",
    "    data (pd.DataFrame): The DataFrame containing the column names.\n",
    "\n",
    "    Returns:\n",
    "    list[tuple[str, str, str]]: A list of tuples, where each tuple contains the column names for June (_6), July (_7), and August (_8).\n",
    "    \"\"\"\n",
    "    columns_6 = [col for col in data.columns if col.endswith('_6')]\n",
    "    columns_7 = [col for col in data.columns if col.endswith('_7')]\n",
    "    columns_8 = [col for col in data.columns if col.endswith('_8')]\n",
    "\n",
    "    monthly_columns = []\n",
    "    for col_6 in columns_6:\n",
    "        base_name = col_6[:-2]\n",
    "        col_7 = f'{base_name}_7'\n",
    "        col_8 = f'{base_name}_8'\n",
    "        if col_7 in columns_7 and col_8 in columns_8:\n",
    "            monthly_columns.append((col_6, col_7, col_8))\n",
    "\n",
    "    return monthly_columns\n",
    "\n",
    "def plot_usage_volume(data: pd.DataFrame, col_june: str, col_july: str, col_aug: str) -> None:\n",
    "    \"\"\"\n",
    "    Plot line graphs showing average June, July, and August volume, split by churn and non-churn.\n",
    "\n",
    "    Parameters:\n",
    "    data (pd.DataFrame): The DataFrame containing user data, including a unique identifier for each respondent and a churn indicator.\n",
    "    col_june (str): The column name for June data.\n",
    "    col_july (str): The column name for July data.\n",
    "    col_aug (str): The column name for August data.\n",
    "    \"\"\"\n",
    "    # Split data into churned and non-churned users\n",
    "    churned = data[data['churn_probability'] == 1]\n",
    "    non_churned = data[data['churn_probability'] == 0]\n",
    "\n",
    "    # Calculate mean volumes for June, July, and August for churned and non-churned users\n",
    "    june_churned_mean = churned[col_june].mean()\n",
    "    july_churned_mean = churned[col_july].mean()\n",
    "    aug_churned_mean = churned[col_aug].mean()\n",
    "\n",
    "    june_non_churned_mean = non_churned[col_june].mean()\n",
    "    july_non_churned_mean = non_churned[col_july].mean()\n",
    "    aug_non_churned_mean = non_churned[col_aug].mean()\n",
    "\n",
    "    # Prepare data for plotting\n",
    "    categories = ['June', 'July', 'August']\n",
    "    churned_values = [june_churned_mean, july_churned_mean, aug_churned_mean]\n",
    "    non_churned_values = [june_non_churned_mean, july_non_churned_mean, aug_non_churned_mean]\n",
    "\n",
    "    base_name = col_june[:-2]\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(categories, churned_values, label='Churned', marker='o', color='red')\n",
    "    plt.plot(categories, non_churned_values, label='Non-Churned', marker='o', color='green')\n",
    "\n",
    "    plt.xlabel('Month')\n",
    "    plt.ylabel('Average Usage Volume')\n",
    "    plt.title(f'{base_name}\\nAverage Usage Volume Comparison: Churned vs Non-Churned Users')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db227258",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns = data.select_dtypes(include=['number']).columns       # We are only interested in volume baed data for the next plots. \n",
    "column_groups = get_monthly_column_groups (data=data[numeric_columns])  # Get monthly KPI as tuples of June, July and August in that order.\n",
    "print (f'Plotting {len(column_groups)} KPI of pairs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1b13a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in column_groups:\n",
    "    #---------------------------------------------\n",
    "    # Plotting\n",
    "    #--------------- ------------------------------\n",
    "    print (f'{c[0][:-2]}')\n",
    "    plot_usage_volume(\n",
    "        data=data,\n",
    "        col_june=c[0],\n",
    "        col_july=c[1],\n",
    "        col_aug=c[2]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fddcaa",
   "metadata": {},
   "source": [
    "## Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "14b1f700",
   "metadata": {},
   "outputs": [],
   "source": [
    "def outlier_handling(dataframe, method='log', cap_value=None):\n",
    "    \"\"\"\n",
    "    Applies outlier handling transformations to the values in the given DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    - dataframe (DataFrame or Series): The DataFrame or Series containing the values to be transformed.\n",
    "    - method (str): The method to handle outliers. Can be 'log', 'clip', or 'none'.\n",
    "                    - 'log' applies log(1 + x) for positive values.\n",
    "                    - 'clip' clips the values to a specified upper limit (cap_value).\n",
    "                    - 'none' leaves the values unchanged (except for handling negative or zero values).\n",
    "    - cap_value (float, optional): The upper limit for clipping. Only used if method='clip'.\n",
    "    \n",
    "    Returns:\n",
    "    - transformed_values (list): A list containing the transformed values based on the specified conditions.\n",
    "    \"\"\"\n",
    "    \n",
    "    transformed_values = []\n",
    "    \n",
    "    for value in dataframe:\n",
    "        if value <= 0:\n",
    "            transformed_values.append(float(0))  # Transform negative and zero values to 0\n",
    "        else:\n",
    "            if method == 'log':\n",
    "                # Apply log transformation for positive values\n",
    "                transformed_values.append(float(np.log1p(value)))  # log(1 + value)\n",
    "            elif method == 'clip' and cap_value is not None:\n",
    "                # Clip values to a specified cap (handles extreme outliers)\n",
    "                transformed_values.append(float(min(value, cap_value)))\n",
    "            else:\n",
    "                # No transformation (optional pass-through)\n",
    "                transformed_values.append(float(value))\n",
    "    \n",
    "    return transformed_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749e055b",
   "metadata": {},
   "outputs": [],
   "source": [
    "detailed_summary(data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97dc1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_columns = list(data.select_dtypes(include=['number']))\n",
    "numerical_columns.remove('id')\n",
    "numerical_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "40c2c96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[numerical_columns] = data[numerical_columns].apply(outlier_handling)\n",
    "unseen[numerical_columns] = unseen[numerical_columns].apply(outlier_handling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a054cc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape, unseen.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fac06be",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfdf0f7",
   "metadata": {},
   "source": [
    "# Feature Egineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890294b6",
   "metadata": {},
   "source": [
    "### Capture change in arpu and recharge amount. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d15c853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for calculating changes\n",
    "def calculate_arpu_change(row):\n",
    "    if pd.notna(row['arpu_8']) and pd.notna(row['arpu_7']):\n",
    "        return (row['arpu_8'] - row['arpu_7']) / (abs(row['arpu_7']) + 1e-5)\n",
    "    elif pd.notna(row['arpu_7']) and pd.notna(row['arpu_6']):\n",
    "        return (row['arpu_7'] - row['arpu_6']) / (abs(row['arpu_6']) + 1e-5)\n",
    "    elif pd.notna(row['arpu_8']) and pd.notna(row['arpu_6']):\n",
    "        return (row['arpu_8'] - row['arpu_6']) / (abs(row['arpu_6']) + 1e-5)\n",
    "    return 0\n",
    "\n",
    "def calculate_recharge_change(row):\n",
    "    if pd.notna(row['total_rech_amt_8']) and pd.notna(row['total_rech_amt_7']):\n",
    "        return (row['total_rech_amt_8'] - row['total_rech_amt_7']) / (abs(row['total_rech_amt_7']) + 1e-5)\n",
    "    elif pd.notna(row['total_rech_amt_7']) and pd.notna(row['total_rech_amt_6']):\n",
    "        return (row['total_rech_amt_7'] - row['total_rech_amt_6']) / (abs(row['total_rech_amt_6']) + 1e-5)\n",
    "    elif pd.notna(row['total_rech_amt_8']) and pd.notna(row['total_rech_amt_6']):\n",
    "        return (row['total_rech_amt_8'] - row['total_rech_amt_6']) / (abs(row['total_rech_amt_6']) + 1e-5)\n",
    "    return 0\n",
    "\n",
    "# Calculate and add new features\n",
    "for df in [data, unseen]:\n",
    "    df.loc[:, 'latest_arpu_change'] = df.apply(calculate_arpu_change, axis=1)\n",
    "    df.loc[:, 'latest_rech_amt_change'] = df.apply(calculate_recharge_change, axis=1)\n",
    "    df.drop(columns=['arpu_6', 'arpu_7', 'arpu_8', 'total_rech_amt_6', 'total_rech_amt_7', 'total_rech_amt_8'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0df81d5",
   "metadata": {},
   "source": [
    "`Further work has revealed slope does describe overall data well but it does not define edge cases well and is not helping model score. A better defination of score is required which is calculated after better handling of edge cases like nagetative arpu. But given the nature of work which is student work, we will not spending more time on this`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138611c8",
   "metadata": {},
   "source": [
    "`We are no longer calculating slope as new feature`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4aa1ad",
   "metadata": {},
   "source": [
    "## Slope\n",
    "\n",
    "We have seen decresing trend across months for churn users v/s increasing or stable trend for non-churn users.\n",
    "We will capture this trend as slope for each monthly KPI\n",
    "\n",
    "Though trend is not significant for few KPI, we will calculate slope for all variables and tackle extra variable during feature selection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4a28e37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_in_usages(data: pd.DataFrame, col_june: str, col_july: str, col_aug: str) -> Tuple[pd.Series, str]:\n",
    "    \"\"\"\n",
    "    Calculate the rolling slope of usage volume between June, July, and August to capture trends.\n",
    "\n",
    "    Parameters:\n",
    "    data (pd.DataFrame): The DataFrame containing the usage data.\n",
    "    col_june (str): The column name for June data.\n",
    "    col_july (str): The column name for July data.\n",
    "    col_aug (str): The column name for August data.\n",
    "\n",
    "    Returns:\n",
    "    pd.Series with slope\n",
    "    \"\"\"\n",
    "    # Create a DataFrame containing the usage columns for June, July, and August\n",
    "    usage_df = data[[col_june, col_july, col_aug]]\n",
    "    \n",
    "    base_name = f'Slope_{col_june[:-2]}'\n",
    "\n",
    "    # Calculate the slope using numpy's polyfit function (degree 1 for linear fit)\n",
    "    slope = usage_df.apply(lambda row: np.polyfit([1, 2, 3], row, 1)[0], axis=1)\n",
    "    # print (base_name, flush=True)\n",
    "    return slope, base_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a81ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "temp_data_main = pd.DataFrame() # Empty data to hold new slope variables.\n",
    "columns_to_drop_main = []\n",
    "for c in column_groups:\n",
    "    # Calculate slop\n",
    "    # slope, col_name = change_in_usages(\n",
    "    slope, col_name = change_in_usages_v2(\n",
    "        data=data,\n",
    "        col_june=c[0],\n",
    "        col_july=c[1],\n",
    "        col_aug=c[2]\n",
    "    )\n",
    "    temp_data_main[col_name] = slope\n",
    "    columns_to_drop_main.append(c[0])\n",
    "    columns_to_drop_main.append(c[1])\n",
    "    columns_to_drop_main.append(c[2])\n",
    "\n",
    "data = data.drop(columns=columns_to_drop_main)\n",
    "data = pd.concat([data, temp_data_main], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408bcd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "temp_data_unseen = pd.DataFrame() # Empty dataframe \n",
    "columns_to_drop_unseen = []\n",
    "for c in column_groups:\n",
    "    # Calculate slop for unseen \n",
    "    # slope, col_name = change_in_usages(\n",
    "    slope, col_name = change_in_usages_v2(\n",
    "        data=unseen,\n",
    "        col_june=c[0],\n",
    "        col_july=c[1],\n",
    "        col_aug=c[2]\n",
    "    )\n",
    "    temp_data_unseen[col_name] = slope\n",
    "    columns_to_drop_unseen.append(c[0])\n",
    "    columns_to_drop_unseen.append(c[1])\n",
    "    columns_to_drop_unseen.append(c[2])\n",
    "\n",
    "unseen = unseen.drop(columns=columns_to_drop_unseen)\n",
    "unseen = pd.concat([unseen, temp_data_unseen], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "43f9abce",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(columns_to_drop_unseen == columns_to_drop_main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ffbd0682",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(data.shape[1] == unseen.shape[1]+1) # Unseen has one less column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3110be",
   "metadata": {},
   "source": [
    "## One-hot encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c642cc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = list(data.select_dtypes(include=['category']).columns)\n",
    "categorical_columns.remove('churn_probability') # Already in the right format.\n",
    "categorical_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "00f88a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.get_dummies(data, columns=categorical_columns, drop_first=True, dtype=float)\n",
    "unseen = pd.get_dummies(unseen, columns=categorical_columns, drop_first=True, dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780db1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape, unseen.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "34ac4408",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_pickle('data.pkl')\n",
    "unseen.to_pickle('unseen.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "0446b0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle('data.pkl')\n",
    "unseen = pd.read_pickle('unseen.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1566a22f",
   "metadata": {},
   "source": [
    "# Feature selection\n",
    "\n",
    "We are struggling with feature selection and in absence of guidance not able to make business focused decision.\n",
    "\n",
    "We wil use techniques to select the most suitable features for predication "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a707bcc",
   "metadata": {},
   "source": [
    "## Feature selection using correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "13f965ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the features and target in the `data` DataFrame\n",
    "X = data.drop(columns=['churn_probability'])  # Features (exclude target)\n",
    "y = data['churn_probability']                 # Target\n",
    "\n",
    "# Compute the correlation matrix for the feature matrix X\n",
    "corr_matrix = X.corr().abs()\n",
    "\n",
    "# Identify features with high correlation (e.g., above 0.9)\n",
    "upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > 0.9)]\n",
    "\n",
    "# Drop highly correlated features from both the data and unseen DataFrames\n",
    "X_reduced = X.drop(columns=to_drop)  # Reduced feature set for data\n",
    "unseen_reduced = unseen.drop(columns=to_drop)  # Reduced feature set for unseen data\n",
    "\n",
    "# Add the target variable back to the `data` DataFrame\n",
    "X_reduced['churn_probability'] = y\n",
    "\n",
    "# Assign the reduced DataFrames back to `data` and `unseen`\n",
    "data = X_reduced.copy(deep=True)\n",
    "unseen = unseen_reduced.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30f6494",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape, unseen.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d919ec86",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(data.shape[1] == unseen.shape[1]+1) # Unseen has one less column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5ddf0d",
   "metadata": {},
   "source": [
    "## Feature selection using RandomForestClassifier (top 50 features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "129b227a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the features and target in the `data` DataFrame\n",
    "X = data.drop(columns=['churn_probability'])  # Features (exclude target)\n",
    "y = data['churn_probability']                # Target\n",
    "\n",
    "# Train a RandomForestClassifier\n",
    "rf = RandomForestClassifier(random_state=42, class_weight='balanced')\n",
    "rf.fit(X, y)\n",
    "\n",
    "# Get feature importances from the trained model\n",
    "importances = rf.feature_importances_\n",
    "\n",
    "# Sort features by importance\n",
    "importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': importances})\n",
    "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Select top N features (for example, top 50)\n",
    "top_n = 50  # Set this value based on your requirement\n",
    "top_features = importance_df['Feature'].head(top_n).tolist()\n",
    "\n",
    "# Reduce the feature set in both `data` and `unseen` DataFrames\n",
    "X_reduced = X[top_features].copy()  # Reduced features for data, use `.copy()` to avoid warnings\n",
    "unseen_reduced = unseen[top_features].copy()  # Reduced features for unseen data\n",
    "\n",
    "# Add the target variable back to the `data` DataFrame using .loc\n",
    "X_reduced.loc[:, 'churn_probability'] = y\n",
    "\n",
    "# Assign the reduced DataFrames back to `data` and `unseen`\n",
    "data = X_reduced.copy(deep=True)\n",
    "unseen = unseen_reduced.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb6ba88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns selected by RandomForestClassifier \n",
    "list(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8917e0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape, unseen.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "bd2c50bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(data.shape[1] == unseen.shape[1]+1) # Unseen has one less column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaedbeb2",
   "metadata": {},
   "source": [
    "# Train - test split and SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2b965d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(columns=['churn_probability'])\n",
    "y = data['churn_probability']\n",
    "\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83051658",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "X_train.shape, X_val.shape, y_train.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9867ff9a",
   "metadata": {},
   "source": [
    "# Handling data imbalance using SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b638b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts() / len (y_train) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a016f2d",
   "metadata": {},
   "source": [
    "We have imbalance in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "06366a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from imblearn.over_sampling import KMeansSMOTE\n",
    "# kmeans_smote = KMeansSMOTE(sampling_strategy=0.5, random_state=42, kmeans_estimator=5, cluster_balance_threshold=0.1)\n",
    "# X_resampled, y_resampled = kmeans_smote.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bcffc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "y_resampled.value_counts() / len (y_resampled) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f75e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_resampled.shape, y_resampled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "4fc2caa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_resampled.to_pickle('X_resampled.pkl')\n",
    "y_resampled.to_pickle('y_resampled.pkl')\n",
    "X_val.to_pickle('X_val.pkl')\n",
    "y_val.to_pickle('y_val.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "110cd565",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_resampled = pd.read_pickle('X_resampled.pkl')\n",
    "y_resampled = pd.read_pickle('y_resampled.pkl')\n",
    "X_val = pd.read_pickle('X_val.pkl')\n",
    "y_val = pd.read_pickle('y_val.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "47ca0229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handy function to write submisson file. \n",
    "def create_submission(id : pd.Series, prediction : pd.Series, model_name : str) -> None:\n",
    "    submission = pd.DataFrame({'id': id.astype(int), 'churn_probability': prediction})\n",
    "    submission.to_csv(f'submission_{model_name}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "419630a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "def plot_roc_curve(y_true : pd.Series, y_pred_prob : pd.Series, title : str = ''):\n",
    "    \"\"\"\n",
    "    Plots the ROC curve for the given true labels and predicted probabilities.\n",
    "\n",
    "    Parameters:\n",
    "    y_true (array-like): True binary labels (0s and 1s).\n",
    "    y_pred_prob (array-like): Predicted probabilities for the positive class (1).\n",
    "    \n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Calculate the ROC curve and AUC\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_pred_prob)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    # Plotting the ROC curve\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, color='blue', label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='red', linestyle='--')  # Diagonal line\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'ROC Curve {title}')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad53848f",
   "metadata": {},
   "source": [
    "# PCA for Determining Critical Numeber of Components\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18b0b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the data before applying PCA\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_resampled)  # Assuming X_resampled is the feature data\n",
    "\n",
    "# Apply PCA without specifying n_components to capture all components\n",
    "pca = PCA()\n",
    "pca.fit(X_scaled)\n",
    "\n",
    "# Get the explained variance ratio\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "\n",
    "# Plot the explained variance to visualize how much variance each component explains\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(range(1, len(explained_variance) + 1), explained_variance, marker='o', linestyle='--')\n",
    "plt.title('Explained Variance by Principal Components')\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Calculate the cumulative explained variance\n",
    "cumulative_explained_variance = pca.explained_variance_ratio_.cumsum()\n",
    "\n",
    "# Plot the cumulative explained variance to identify the number of components needed\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(range(1, len(cumulative_explained_variance) + 1), cumulative_explained_variance, marker='o', linestyle='--')\n",
    "plt.title('Cumulative Explained Variance by Principal Components')\n",
    "plt.xlabel('Number of Principal Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Find the number of components that explain at least 90% of the variance\n",
    "n_components_80 = next(i for i, total_var in enumerate(cumulative_explained_variance) if total_var >= 0.80) + 1\n",
    "n_components_85 = next(i for i, total_var in enumerate(cumulative_explained_variance) if total_var >= 0.85) + 1\n",
    "n_components_90 = next(i for i, total_var in enumerate(cumulative_explained_variance) if total_var >= 0.90) + 1\n",
    "n_components_95 = next(i for i, total_var in enumerate(cumulative_explained_variance) if total_var >= 0.95) + 1\n",
    "n_components_99 = next(i for i, total_var in enumerate(cumulative_explained_variance) if total_var >= 0.99) + 1\n",
    "\n",
    "print(f'Number of components explaining at least 80% of the variance: {n_components_80}')\n",
    "print(f'Number of components explaining at least 85% of the variance: {n_components_85}')\n",
    "print(f'Number of components explaining at least 90% of the variance: {n_components_90}')\n",
    "print(f'Number of components explaining at least 95% of the variance: {n_components_95}')\n",
    "print(f'Number of components explaining at least 99% of the variance: {n_components_99}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61262652",
   "metadata": {},
   "source": [
    "# Model 1 - Logistic Regression with RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31076ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Define the pipeline without GridSearchCV\n",
    "logreg_rfe_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()), \n",
    "    ('rfe', RFE(estimator=LogisticRegression(random_state=42, max_iter=500), n_features_to_select=20)),\n",
    "    ('logreg', LogisticRegression(random_state=42, max_iter=500, C=1.0, penalty='l2', solver='saga'))\n",
    "])\n",
    "\n",
    "# Fit the pipeline directly on the resampled data\n",
    "logreg_rfe_pipeline.fit(X_resampled, y_resampled)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_val_pred = logreg_rfe_pipeline.predict(X=X_val)\n",
    "\n",
    "# Evaluate the model using classification report and accuracy score\n",
    "print('Classification report for model : Logreg with RFE\\n',  classification_report(y_val, y_val_pred))\n",
    "\n",
    "# Plot ROC curve\n",
    "y_val_pred_prob = logreg_rfe_pipeline.predict_proba(X_val)\n",
    "plot_roc_curve(y_val, y_val_pred_prob[:, 1], 'Logreg with RFE')\n",
    "\n",
    "print (f'RFE Top 20 features:\\n{list(X_resampled.columns[logreg_rfe_pipeline[\"rfe\"].support_])}\\n')\n",
    "\n",
    "# Create Kaggle submission\n",
    "y_unseen_pred = logreg_rfe_pipeline.predict(X=unseen)\n",
    "create_submission(id=unseen['id'], prediction=y_unseen_pred, model_name='logreg_rfe_pipeline_no_grid_search')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255e7fe0",
   "metadata": {},
   "source": [
    "- **Model Overview**: Logistic Regression with RFE, tuned using GridSearchCV.\n",
    "- **Hyperparameters**: RFE for feature selection, using Logistic Regression.\n",
    "- **Variables Selected by RFE (Top 20)**:\n",
    "  - 'total_ic_mou_8', 'roam_ic_mou_8', 'last_day_rch_amt_8', 'arpu_8', 'total_rech_amt_8', 'loc_ic_t2m_mou_8', 'loc_og_t2m_mou_8', 'vol_2g_mb_8', 'total_rech_num_8', 'total_ic_mou_7', 'vol_3g_mb_8', 'total_amt_7', 'aon', 'loc_og_t2m_mou_7', 'total_rech_amt_7', 'total_rech_num_7', 'offnet_mou_7', 'total_rech_amt_6', 'avg_rech_amt_6_7', 'onnet_mou_6'\n",
    "- **Performance Summary**:\n",
    "  - **Class 0 (Non-churn)**: High precision (0.98) and reasonable recall (0.89), resulting in a strong F1-score of 0.94.\n",
    "  - **Class 1 (Churn)**: Precision is low (0.39), while recall is moderate (0.79). The F1-score of 0.52 indicates that the model has difficulty correctly predicting churn cases, with a significant number of false positives.\n",
    "  - **Overall Metrics**:\n",
    "    - Accuracy: 89%\n",
    "    - The macro F1-score is 0.73, which is similar to the PCA version but still demonstrates a weakness in identifying churn cases effectively.\n",
    "- **ROC Curve**: AUC of 0.92, indicating that the model has some capability to distinguish between churn and non-churn, but not as strong as more complex models.\n",
    "- **Observation**:\n",
    "  - The RFE-selected variables are related to recent activity (e.g., total incoming calls, recharge amounts, and data usage). However, the model still struggles with precision for the churn class (1), resulting in many false positives. This impacts the reliability of churn predictions. Exploring more advanced feature selection methods or moving to a non-linear model might help improve the performance for correctly identifying churn cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc32cc9",
   "metadata": {},
   "source": [
    "# Model 2 - Logistic Regression with PCA, RFE and Hyperparameter Tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea71b9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Create a pipeline with StandardScaler, PCA, RFE, and LogisticRegression\n",
    "logreg_pca_rfe_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()), \n",
    "    ('pca', PCA(n_components=n_components_90)),\n",
    "    ('rfe', RFE(estimator=LogisticRegression(random_state=42), n_features_to_select=20)),\n",
    "    ('logreg', LogisticRegression(random_state=42))\n",
    "])\n",
    "\n",
    "# Define a parameter grid for Logistic Regression\n",
    "param_grid = {\n",
    "    'logreg__C': [0.001, 0.01, 0.1, 1, 10],       # Regularization strength\n",
    "    'pca__n_components': [12, 15, 20],            # Number of PCA components to try\n",
    "    'rfe__n_features_to_select': [10, 15, 20]     # RFE feature selection tuning\n",
    "}\n",
    "\n",
    "# Create the GridSearchCV object\n",
    "grid_search_logreg = GridSearchCV(logreg_pca_rfe_pipeline, param_grid, cv=5, scoring='precision', n_jobs=-1)\n",
    "\n",
    "# Fit the model using GridSearchCV\n",
    "grid_search_logreg.fit(X_resampled, y_resampled)\n",
    "\n",
    "# Evaluate the best model on the validation set\n",
    "y_val_pred = grid_search_logreg.predict(X_val)\n",
    "\n",
    "# Evaluate the model using classification report and accuracy score\n",
    "print('Classification report for model : Logreg with PCA and RFE\\n',  classification_report(y_val, y_val_pred))\n",
    "\n",
    "# Plot ROC curve for the best model\n",
    "y_val_pred_prob = grid_search_logreg.best_estimator_.predict_proba(X_val)\n",
    "plot_roc_curve(y_val, y_val_pred_prob[:, 1], 'Logreg with PCA and RFE')\n",
    "\n",
    "# Print best config\n",
    "print (f'Best params :\\n {grid_search_logreg.best_params_}\\n')\n",
    "\n",
    "# Make predictions on the unseen data\n",
    "y_unseen_pred = grid_search_logreg.best_estimator_.predict(unseen)\n",
    "create_submission(id=unseen['id'], prediction=y_unseen_pred, model_name='logreg_pca_rfe_tuned')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24acee2",
   "metadata": {},
   "source": [
    "- **Model Overview**: Logistic Regression with PCA and RFE, tuned using GridSearchCV.\n",
    "- **Hyperparameters**:\n",
    "  - PCA components: 20\n",
    "  - RFE features selected: 15\n",
    "  - Logistic Regression parameter: `C` = 0.001\n",
    "- **Performance Summary**:\n",
    "  - **Class 0 (Non-churn)**: High precision (0.98) and reasonable recall (0.89), resulting in a strong F1-score of 0.93.\n",
    "  - **Class 1 (Churn)**: Very low precision (0.38) but moderate recall (0.76). This results in a poor F1-score of 0.50, indicating the model struggles to accurately classify churn cases.\n",
    "  - **Overall Metrics**:\n",
    "    - Accuracy: 88%\n",
    "    - The macro F1-score is 0.72, highlighting significant challenges in predicting the minority class effectively.\n",
    "- **ROC Curve**: AUC of 0.91, indicating that the model's ability to distinguish between churn and non-churn classes is weaker compared to previous models.\n",
    "- **Observation**:\n",
    "  - The model's poor precision for churn class (1) indicates a significant issue with false positives, meaning many non-churn customers are incorrectly labeled as churn. This could be a concern for practical application. Increasing the number of features selected or using a more complex model could potentially improve performance for the minority class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb8135c",
   "metadata": {},
   "source": [
    "# Model 3 - Decision Tree with PCA and GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c386a2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Define the pipeline with StandardScaler, PCA, and DecisionTreeClassifier\n",
    "dt_pca_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()), \n",
    "    ('pca', PCA(n_components=n_components_90)),\n",
    "    ('dt', DecisionTreeClassifier(random_state=42))  # Decision Tree classifier\n",
    "])\n",
    "\n",
    "# Simplified parameter grid based on research insights\n",
    "param_grid = {\n",
    "    'pca__n_components': [12, 15, 20],        # Number of PCA components to try\n",
    "    # 'dt__max_depth': [10, 15],              # Shallower trees help reduce overfitting; limit to 10 or 15\n",
    "    'dt__min_samples_leaf': [10, 20],         # Larger minimum samples help generalize better in telecom churn data\n",
    "    'dt__criterion': ['gini', 'entropy']                 # Stick with 'gini', commonly used and optimal for churn prediction\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV with DecisionTree and the reduced parameter grid\n",
    "grid_search_dt = GridSearchCV(estimator=dt_pca_pipeline, param_grid=param_grid, cv=5, scoring='precision', n_jobs=-1)\n",
    "\n",
    "# Fit the grid search on the resampled data\n",
    "grid_search_dt.fit(X_resampled, y_resampled)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_val_pred = grid_search_dt.predict(X_val)\n",
    "\n",
    "# Evaluate the model using classification report and accuracy score\n",
    "print('Classification report for model : DecisionTree with PCA\\n',  classification_report(y_val, y_val_pred))\n",
    "\n",
    "# Plot ROC curve\n",
    "y_val_pred_prob = grid_search_dt.predict_proba(X_val)\n",
    "plot_roc_curve(y_val, y_val_pred_prob[:, 1], 'DecisionTree with PCA')\n",
    "\n",
    "# Print best config\n",
    "print (f'Best params :\\n {grid_search_dt.best_params_}\\n')\n",
    "\n",
    "# Create Kaggle submission with predictions for unseen data\n",
    "y_unseen_pred = grid_search_dt.predict(X=unseen)\n",
    "create_submission(id=unseen['id'], prediction=y_unseen_pred, model_name='dt_pca_pipeline')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c50df4",
   "metadata": {},
   "source": [
    "- **Model Overview**: DecisionTree with PCA, tuned using GridSearchCV.\n",
    "- **Hyperparameters**:\n",
    "  - PCA components: 20\n",
    "  - DecisionTree parameters:\n",
    "    - `criterion`: 'entropy'\n",
    "    - `min_samples_leaf`: 10\n",
    "- **Performance Summary**:\n",
    "  - **Class 0 (Non-churn)**: High precision (0.99) and recall (0.95), resulting in an F1-score of 0.97, similar to the XGBoost model.\n",
    "  - **Class 1 (Churn)**: Precision remains at 0.60, and recall is 0.89, resulting in an F1-score of 0.72. The performance for this class is consistent with the XGBoost model, indicating that false positives are still an issue.\n",
    "  - **Overall Metrics**: \n",
    "    - Accuracy: 94%\n",
    "    - Macro average F1-score is slightly lower at 0.84, indicating similar issues with class imbalance as seen previously.\n",
    "- **ROC Curve**: AUC of 0.99 shows strong model performance in distinguishing between churn and non-churn.\n",
    "- **Observation**:\n",
    "  - The DecisionTree model also struggles with false positives in predicting churn cases. It performs slightly worse than XGBoost in terms of overall accuracy, but the AUC remains high, suggesting good discriminatory ability. Additional feature engineering or ensemble methods may be helpful in improving precision for class '1'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3d39d3",
   "metadata": {},
   "source": [
    "# Model 4 - Random Forest with PCA and GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76864ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Define the pipeline with StandardScaler, PCA, and RandomForestClassifier\n",
    "rf_pca_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()), \n",
    "    ('pca', PCA(n_components=n_components_90)),  \n",
    "    ('rf', RandomForestClassifier(random_state=42)) \n",
    "])\n",
    "\n",
    "# Simplified parameter grid based on research insights\n",
    "param_grid = {\n",
    "    'pca__n_components': [15, 20],        # Number of PCA components to try\n",
    "    'rf__n_estimators': [100, 200],           # Limit number of trees to 100 or 200 (typically for telecom data)\n",
    "    'rf__max_depth': [10, 20],                # Shallower trees (10-20) to avoid overfitting\n",
    "    'rf__min_samples_split': [2, 5],          # Common settings, reduces overfitting without too much complexity\n",
    "    'rf__min_samples_leaf': [1, 2]            # Minimum leaf size - keep smaller for better churn prediction\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV with RandomForestClassifier and the reduced parameter grid\n",
    "grid_search_rf = GridSearchCV(estimator=rf_pca_pipeline, param_grid=param_grid, cv=5, scoring='precision', n_jobs=-1)\n",
    "\n",
    "# Fit the grid search on the resampled data\n",
    "grid_search_rf.fit(X_resampled, y_resampled)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_val_pred = grid_search_rf.predict(X_val)\n",
    "\n",
    "# Evaluate the model using classification report and accuracy score\n",
    "print('Classification report for model : RandomForest with PCA\\n',  classification_report(y_val, y_val_pred))\n",
    "\n",
    "# Plot ROC curve\n",
    "y_val_pred_prob = grid_search_rf.predict_proba(X_val)\n",
    "plot_roc_curve(y_val, y_val_pred_prob[:, 1], 'RandomForest with PCA')\n",
    "\n",
    "# Print best config\n",
    "print (f'Best params :\\n {grid_search_rf.best_params_}\\n')\n",
    "\n",
    "# Create Kaggle submission with predictions for unseen data\n",
    "y_unseen_pred = grid_search_rf.predict(X=unseen)\n",
    "create_submission(id=unseen['id'], prediction=y_unseen_pred, model_name='rf_pca_pipeline')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86797341",
   "metadata": {},
   "source": [
    "- **Model Overview**: RandomForest with PCA, tuned using GridSearchCV.\n",
    "- **Hyperparameters**:\n",
    "  - PCA components: 20\n",
    "  - RandomForest parameters:\n",
    "    - `max_depth`: 20\n",
    "    - `min_samples_leaf`: 2\n",
    "    - `min_samples_split`: 5\n",
    "    - `n_estimators`: 100\n",
    "- **Performance Summary**:\n",
    "  - **Class 0 (Non-churn)**: High precision, recall, and F1-score, indicating excellent prediction accuracy.\n",
    "  - **Class 1 (Churn)**: Slightly lower precision but very high recall, meaning most churn cases are being correctly identified.\n",
    "  - **Overall Metrics**: \n",
    "    - Accuracy: 98%\n",
    "    - Strong balance between precision and recall, evident from F1-scores.\n",
    "- **ROC Curve**: AUC of 1.00, demonstrating near-perfect separation between churn and non-churn classes.\n",
    "- **Observation**:\n",
    "  - Slightly lower precision for churn class (1) indicates some false positives, which could impact actions based on customer churn predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaed7072",
   "metadata": {},
   "source": [
    "# Model 5 - XGBoost with PCA and GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29da2bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Define the pipeline with StandardScaler, PCA, and XGBClassifier\n",
    "xgb_pca_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()), \n",
    "    ('pca', PCA()),  # PCA without fixed n_components so we can control it through param_grid\n",
    "    ('xgb', XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss'))  # XGBoost classifier\n",
    "])\n",
    "\n",
    "# Simplified parameter grid with different PCA components\n",
    "param_grid = {\n",
    "    'pca__n_components': [13, 16,20, 25],       # Number of PCA components to try\n",
    "    'xgb__n_estimators': [100, 200],            # Limit the number of boosting rounds (200 should be sufficient for our data)\n",
    "    'xgb__max_depth': [3, 5],                   # Tree depth around 3-5 tends to generalize well\n",
    "    'xgb__learning_rate': [0.1],                # 0.1 is a balanced learning rate commonly used in telecom churn models\n",
    "    'xgb__subsample': [0.8],                    # Use a constant subsample ratio of 0.8 for less overfitting\n",
    "    'xgb__colsample_bytree': [0.8],             # Column sampling ratio of 0.8 works well for moderately high-dimensional data\n",
    "    'xgb__min_child_weight': [1]                # Child weight of 1 often provides the right balance for handling noise in telecom data\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV with XGBClassifier and the modified parameter grid\n",
    "grid_search_xgb = GridSearchCV(estimator=xgb_pca_pipeline, param_grid=param_grid, cv=5, scoring='precision', n_jobs=-1)\n",
    "\n",
    "# Fit the grid search on the resampled data\n",
    "grid_search_xgb.fit(X_resampled, y_resampled)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_val_pred = grid_search_xgb.predict(X_val)\n",
    "\n",
    "# Evaluate the model using classification report and accuracy score\n",
    "print('Classification report for model : XGBoost with PCA\\n',  classification_report(y_val, y_val_pred))\n",
    "\n",
    "# Plot ROC curve\n",
    "y_val_pred_prob = grid_search_xgb.predict_proba(X_val)\n",
    "plot_roc_curve(y_val, y_val_pred_prob[:, 1], 'RandomForest with PCA')\n",
    "\n",
    "# Print best config\n",
    "print (f'Best params :\\n {grid_search_xgb.best_params_}\\n')\n",
    "\n",
    "# Create Kaggle submission with predictions for unseen data\n",
    "y_unseen_pred = grid_search_xgb.predict(X=unseen)\n",
    "create_submission(id=unseen['id'], prediction=y_unseen_pred, model_name='XGBoost with PCA')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992b5235",
   "metadata": {},
   "source": [
    "- **Model Overview**: XGBoost with PCA, tuned using GridSearchCV.\n",
    "- **Hyperparameters**:\n",
    "  - PCA components: 25\n",
    "  - XGBoost parameters:\n",
    "    - `colsample_bytree`: 0.8\n",
    "    - `learning_rate`: 0.1\n",
    "    - `max_depth`: 5\n",
    "    - `min_child_weight`: 1\n",
    "    - `n_estimators`: 200\n",
    "    - `subsample`: 0.8\n",
    "- **Performance Summary**:\n",
    "  - **Class 0 (Non-churn)**: Very high precision (0.99) and recall (0.95), resulting in a strong F1-score of 0.97.\n",
    "  - **Class 1 (Churn)**: Precision is relatively low (0.60), while recall is high (0.89). This indicates that the model is good at capturing churn cases, but there are a significant number of false positives.\n",
    "  - **Overall Metrics**: \n",
    "    - Accuracy: 95%\n",
    "    - Imbalance in precision and recall between classes results in a macro F1-score of 0.85, indicating room for improvement, particularly in reducing false positives.\n",
    "- **ROC Curve**: AUC of 0.98 shows that the model has good discriminatory power between the churn and non-churn classes.\n",
    "- **Observation**:\n",
    "  - The lower precision for churn class (1) highlights an issue with false positives, which could affect retention efforts. Further tuning or additional features might help in improving the precision for this class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8224a28f",
   "metadata": {},
   "source": [
    "# Kaggle competition model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017bcc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "kdata = pd.read_csv(\"data/train.csv\")\n",
    "kunseen = pd.read_csv(\"data/test.csv\")\n",
    "\n",
    "# Helper functions for calculating changes\n",
    "def calculate_arpu_change(row):\n",
    "    if pd.notna(row['arpu_8']) and pd.notna(row['arpu_7']):\n",
    "        return (row['arpu_8'] - row['arpu_7']) / (abs(row['arpu_7']) + 1e-5)\n",
    "    elif pd.notna(row['arpu_7']) and pd.notna(row['arpu_6']):\n",
    "        return (row['arpu_7'] - row['arpu_6']) / (abs(row['arpu_6']) + 1e-5)\n",
    "    elif pd.notna(row['arpu_8']) and pd.notna(row['arpu_6']):\n",
    "        return (row['arpu_8'] - row['arpu_6']) / (abs(row['arpu_6']) + 1e-5)\n",
    "    return 0\n",
    "\n",
    "def calculate_recharge_change(row):\n",
    "    if pd.notna(row['total_rech_amt_8']) and pd.notna(row['total_rech_amt_7']):\n",
    "        return (row['total_rech_amt_8'] - row['total_rech_amt_7']) / (abs(row['total_rech_amt_7']) + 1e-5)\n",
    "    elif pd.notna(row['total_rech_amt_7']) and pd.notna(row['total_rech_amt_6']):\n",
    "        return (row['total_rech_amt_7'] - row['total_rech_amt_6']) / (abs(row['total_rech_amt_6']) + 1e-5)\n",
    "    elif pd.notna(row['total_rech_amt_8']) and pd.notna(row['total_rech_amt_6']):\n",
    "        return (row['total_rech_amt_8'] - row['total_rech_amt_6']) / (abs(row['total_rech_amt_6']) + 1e-5)\n",
    "    return 0\n",
    "\n",
    "# Calculate and add new features\n",
    "for df in [kdata, kunseen]:\n",
    "    df.loc[:, 'latest_arpu_change'] = df.apply(calculate_arpu_change, axis=1)\n",
    "    df.loc[:, 'latest_rech_amt_change'] = df.apply(calculate_recharge_change, axis=1)\n",
    "\n",
    "# Filter necessary columns\n",
    "kdata = kdata[['latest_arpu_change', 'latest_rech_amt_change', 'aon', 'churn_probability']].copy(deep=True)\n",
    "kunseen = kunseen[['id', 'latest_arpu_change', 'latest_rech_amt_change', 'aon']].copy(deep=True)\n",
    "\n",
    "# Prepare train-test split\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    kdata.drop(columns=['churn_probability']), kdata['churn_probability'], \n",
    "    test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_val_pred = xgb_model.predict(X_val)\n",
    "\n",
    "# Evaluate the model using classification report and accuracy score\n",
    "print('Classification report for model : XGBoost with three features\\n',  classification_report(y_val, y_val_pred))\n",
    "\n",
    "# Plot ROC curve\n",
    "y_val_pred_prob = xgb_model.predict_proba(X_val)\n",
    "plot_roc_curve(y_val, y_val_pred_prob[:, 1], 'Kaggle - XGBoost with three features')\n",
    "\n",
    "\n",
    "# Predict on unseen data and create submission\n",
    "kunseen.loc[:, 'churn_probability'] = xgb_model.predict(kunseen.drop(columns='id'))\n",
    "create_submission(id=kunseen['id'], prediction=kunseen['churn_probability'], model_name='xgb_model_competition')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1e135d",
   "metadata": {},
   "source": [
    "### Kaggle submission Model Performance Summary:\n",
    "\n",
    "The model performs well in predicting **non-churn (Class 0)** cases, with high precision, recall, and F1-score. However, **churn (Class 1)** is more challenging for the model, as indicated by lower recall (0.40), which suggests that many actual churners are not being identified. This is likely because the dataset is imbalanced, and the model has a bias toward the majority class (non-churn).\n",
    "\n",
    "**Macro and Weighted Averages**:\n",
    "- The macro average (averaging the metrics across both classes) for recall is **0.69**, and the F1-score is **0.73**, indicating that the model struggles with the minority class.\n",
    "- The weighted average is better due to the larger support for non-churn cases.\n",
    "\n",
    "**ROC Curve and AUC**:\n",
    "- The ROC curve shows a good separation between classes, with an **AUC (Area Under Curve) of 0.84**, indicating a reasonably strong model in distinguishing between churners and non-churners.\n",
    "- The ROC curve demonstrates good predictive power but suggests room for improvement, especially in minimizing false negatives for churn cases.\n",
    "\n",
    "**Summary**:\n",
    "The model shows **high accuracy** and performs well for the **majority class (non-churn)** but struggles with **minority class (churn)** due to class imbalance. The **AUC of 0.84** suggests a solid model, but the relatively low recall for churn cases (0.40) implies that further improvements, such as balancing techniques or hyperparameter tuning, are needed to enhance recall and overall performance for churn prediction."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
