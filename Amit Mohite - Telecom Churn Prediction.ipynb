{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed5aad45",
   "metadata": {},
   "source": [
    "# Telecom Churn Case Study - Kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d49ca6d",
   "metadata": {},
   "source": [
    "# `References`\n",
    "\n",
    "\n",
    "https://medium.com/analytics-vidhya/telecom-churn-prediction-9ce72c24e961"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a73769",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "This is a student exercise using telecom churn data. The goal is to build a model to predict customer churn probability. The data is provided in two parts: one dataset with known churn outcomes (`train.csv`) and another without churn outcomes (`test.csv`).\n",
    "\n",
    "### Exercise Overview:\n",
    "\n",
    "1. **80% weightage:** Focus on building the best model for the business caseâ€”predicting churn probability based on multiple KPIs.\n",
    "2. **20% weightage:** A submission to a Kaggle competition, where the model is evaluated on unseen data (`test.csv`).\n",
    "\n",
    "The data contains KPIs measured across months, and our EDA will explore how these KPIs evolve over time. We will also test different models to compare their performance and choose the best one for Kaggle submission.\n",
    "\n",
    "The model with the highest accuracy will be selected for the Kaggle submission, even if it is not the best across all evaluation metrics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d42f5a",
   "metadata": {},
   "source": [
    "# Module imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "id": "97992874",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd ; pd.set_option('display.max_rows', 500)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "id": "f0c2102d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.feature_selection import RFE\n",
    "from imblearn.pipeline import make_pipeline\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, roc_auc_score, classification_report, accuracy_score, precision_score\n",
    "from sklearn.decomposition import PCA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab443b51",
   "metadata": {},
   "source": [
    "# Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "id": "baf12fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Any\n",
    "from sklearn.experimental import enable_iterative_imputer  # Required to use IterativeImputer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.impute import IterativeImputer, KNNImputer\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from scipy.stats import zscore\n",
    "\n",
    "def detailed_summary(data: pd.DataFrame, display_top_n=3) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Provides detailed summary for both categorical and numerical variables.\n",
    "\n",
    "    Data    : Pandas dataframe\n",
    "    display_top_n : Number of top frequent values to display for categorical variables.\n",
    "    \n",
    "    Returns :\n",
    "        Pandas dataframe with descriptive summary for both categorical and numerical columns.\n",
    "    \"\"\"\n",
    "    # Separate numerical and categorical columns\n",
    "    categorical_columns = data.select_dtypes(exclude=['int64', 'float64']).columns\n",
    "    numerical_columns = data.select_dtypes(include=['int64', 'float64']).columns\n",
    "    \n",
    "    numerical_summary = pd.DataFrame()\n",
    "    categorical_summary = pd.DataFrame()\n",
    "\n",
    "    if len(numerical_columns) > 0:\n",
    "        # Summary for numerical columns\n",
    "        numerical_summary = data[numerical_columns].describe().T\n",
    "        numerical_summary['missing_values %'] = round(data[numerical_columns].isna().sum() / len(data) * 100, 2)\n",
    "        numerical_summary['skew'] = data[numerical_columns].skew()\n",
    "        numerical_summary['kurtosis'] = data[numerical_columns].kurtosis()\n",
    "        numerical_summary['unique_values'] = data[numerical_columns].nunique(dropna=True)\n",
    "        numerical_summary['single_unique_value'] = data[numerical_columns].apply(lambda x: 'Yes' if x.nunique(dropna=True) == 1 else 'No')\n",
    "\n",
    "    if len(categorical_columns) > 0:\n",
    "        # Summary for categorical columns\n",
    "        categorical_summary = pd.DataFrame(index=categorical_columns)\n",
    "        categorical_summary['missing_values %'] = round(data[categorical_columns].isna().sum() / len(data) * 100, 2)\n",
    "        categorical_summary['unique_values'] = data[categorical_columns].nunique()\n",
    "        categorical_summary['most_frequent'] = data[categorical_columns].mode().iloc[0]\n",
    "        categorical_summary['frequency'] = data[categorical_columns].apply(lambda x: x.value_counts().iloc[0])\n",
    "        # categorical_summary['top_n_frequent'] = data[categorical_columns].apply(\n",
    "        #     lambda x: dict(x.value_counts().head(display_top_n))\n",
    "        # )\n",
    "        categorical_summary['single_unique_value'] = data[categorical_columns].apply(lambda x: 'Yes' if x.nunique() == 1 else 'No')\n",
    "\n",
    "    # Combine both summaries\n",
    "    return pd.concat([numerical_summary, categorical_summary], axis=0)\n",
    "\n",
    "def find_columns_by_string(data : pd.DataFrame, search_string : str):\n",
    "    \"\"\"\n",
    "    find columns which has given search string\n",
    "    \"\"\"\n",
    "    return [col for col in data.columns if search_string.lower() in col.lower()]\n",
    "\n",
    "def find_columns_by_regex(data, regex_pattern):\n",
    "    \"\"\"\n",
    "    find columns which has search string matching regex pattern\n",
    "    \"\"\"\n",
    "    return [col for col in data.columns if re.search(regex_pattern, col, re.IGNORECASE)]\n",
    "\n",
    "def handle_missing_values(data: pd.DataFrame, strategy=\"mean\", impute_categorical=False, knn_neighbors=5) -> Tuple[Any, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Handles missing values in the dataframe based on the provided strategy.\n",
    "    \n",
    "    strategy : str, default \"mean\"\n",
    "        Imputation strategy for numerical columns:\n",
    "        - \"mean\": replaces missing values with mean\n",
    "        - \"median\": replaces missing values with median\n",
    "        - \"most_frequent\": replaces missing values with the most frequent value\n",
    "        - \"knn\": uses KNN imputer\n",
    "        - \"iterative\": uses Iterative imputer (more advanced)\n",
    "    \n",
    "    impute_categorical : bool, default True\n",
    "        Whether to impute categorical variables with the most frequent value.\n",
    "    \n",
    "    knn_neighbors : int, default 5\n",
    "        Number of neighbors to use for KNN imputation.\n",
    "    \n",
    "    Returns:\n",
    "        imputer --> useful for imputing test sets or review model parameters.\n",
    "        DataFrame with missing values handled.\n",
    "    \"\"\"\n",
    "    \n",
    "    data_cleaned = data.copy()\n",
    "    \n",
    "    # Separate numerical and categorical columns\n",
    "    numerical_cols = data_cleaned.select_dtypes(include=['float64', 'int64']).columns\n",
    "    categorical_cols = data_cleaned.select_dtypes(exclude=['float64', 'int64']).columns\n",
    "    \n",
    "    if len(numerical_cols) > 0:\n",
    "        # Handle missing values for numerical columns\n",
    "        if strategy in [\"mean\", \"median\", \"most_frequent\"]:\n",
    "            imputer = SimpleImputer(strategy=strategy)\n",
    "            data_cleaned[numerical_cols] = imputer.fit_transform(data_cleaned[numerical_cols])\n",
    "        \n",
    "        elif strategy == \"knn\":\n",
    "            imputer = KNNImputer(n_neighbors=knn_neighbors, random_state=42)\n",
    "            data_cleaned[numerical_cols] = imputer.fit_transform(data_cleaned[numerical_cols])\n",
    "        \n",
    "        elif strategy == \"iterative\":\n",
    "            imputer = IterativeImputer()\n",
    "            data_cleaned[numerical_cols] = imputer.fit_transform(data_cleaned[numerical_cols])\n",
    "    \n",
    "    # Handle missing values for categorical columns (if applicable)\n",
    "    if len(categorical_cols) > 0 and impute_categorical:\n",
    "        imputer = SimpleImputer(strategy=\"most_frequent\")\n",
    "        data_cleaned[categorical_cols] = imputer.fit_transform(data_cleaned[categorical_cols])\n",
    "    \n",
    "    return imputer, data_cleaned\n",
    "\n",
    "def cap_outliers_isolation_forest(data: pd.DataFrame, columns: list, contamination: float = 0.01) -> Tuple[pd.DataFrame, Any]:\n",
    "    \"\"\"\n",
    "    cap_outliers_isolation_forest\n",
    "    -----------------------------\n",
    "\n",
    "    Caps columns in data using Isolation Forest\n",
    "\n",
    "    data    : Pandas dataframe\n",
    "    columns : list of columns\n",
    "\n",
    "    Returns :\n",
    "        Capped dataframe\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    clf = IsolationForest(contamination=contamination, random_state=42)\n",
    "\n",
    "    for column in columns:\n",
    "        data['outlier_if'] = clf.fit_predict(data[[column]])\n",
    "\n",
    "        non_outliers = data[data['outlier_if'] == 1][column]\n",
    "        lower_bound = non_outliers.min()\n",
    "        upper_bound = non_outliers.max()\n",
    "\n",
    "        data[column] = np.where(data[column] < lower_bound,\n",
    "                              lower_bound, data[column])\n",
    "        data[column] = np.where(data[column] > upper_bound,\n",
    "                              upper_bound, data[column])\n",
    "\n",
    "        data = data.drop(columns=['outlier_if'])\n",
    "        print(f'Capped outliers in {column}')\n",
    "\n",
    "    return data, clf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70483f4",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73bf6f4",
   "metadata": {},
   "source": [
    "### Links to data on google drive \n",
    "\n",
    "- https://drive.google.com/uc?id=1BTXvDT0JQK_bNXEMBJCSl4AWgDlBHMVv - train.csv\n",
    "- https://drive.google.com/uc?id=1NeIn5Y199H1WNYxZbryUwrKdxHw3tjHz - test.csv\n",
    "- https://drive.google.com/uc?id=1cN3iMwxib1a_8POB3DZDf9g9_kQJ-onV - sample.csv\n",
    "- https://drive.google.com/uc?id=11YclgZdYOuJYptrNutqCOoLgf4bz9xL8 - dictionary.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71638237",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "id": "760c09c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_csv(\"data/train.csv\")\n",
    "# unseen = pd.read_csv(\"data/test.csv\")\n",
    "# kaggle_submisson_sample = pd.read_csv(\"data/sample.csv\")\n",
    "# data_dictionary = pd.read_csv(\"data/data_dictionary.csv\")\n",
    "\n",
    "data = pd.read_csv(\"https://drive.google.com/uc?id=1BTXvDT0JQK_bNXEMBJCSl4AWgDlBHMVv\")\n",
    "unseen = pd.read_csv(\"https://drive.google.com/uc?id=1NeIn5Y199H1WNYxZbryUwrKdxHw3tjHz\")\n",
    "kaggle_submisson_sample = pd.read_csv(\"https://drive.google.com/uc?id=1cN3iMwxib1a_8POB3DZDf9g9_kQJ-onV\")\n",
    "data_dictionary = pd.read_csv(\"https://drive.google.com/uc?id=11YclgZdYOuJYptrNutqCOoLgf4bz9xL8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"train.csv\", data.shape)\n",
    "print(\"test.csv\", unseen.shape)\n",
    "print(\"sample.csv\", kaggle_submisson_sample.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0948952d",
   "metadata": {},
   "source": [
    "`data_dictionary.csv`\n",
    "\n",
    "|Acronyms|Description|\n",
    "|---|---|\n",
    "|CIRCLE_ID|Telecom circle area to which the customer belongs to|\n",
    "|LOC|Local calls  within same telecom circle|\n",
    "|STD|STD calls  outside the calling circle|\n",
    "|IC|Incoming calls|\n",
    "|OG|Outgoing calls|\n",
    "|T2T|Operator T to T ie within same operator mobile to mobile|\n",
    "|T2M    |Operator T to other operator mobile|\n",
    "|T2O    |Operator T to other operator fixed line|\n",
    "|T2F    |Operator T to fixed lines of T|\n",
    "|T2C    |Operator T to its own call center|\n",
    "|ARPU    |Average revenue per user|\n",
    "|MOU    |Minutes of usage  voice calls|\n",
    "|AON    |Age on network  number of days the customer is using the operator T network|\n",
    "|ONNET   |All kind of calls within the same operator network|\n",
    "|OFFNET    |All kind of calls outside the operator T network|\n",
    "|ROAM|Indicates that customer is in roaming zone during the call|\n",
    "|SPL   |Special calls|\n",
    "|ISD    |ISD calls|\n",
    "|RECH    |Recharge|\n",
    "|NUM    |Number|\n",
    "|AMT    |Amount in local currency|\n",
    "|MAX    |Maximum|\n",
    "|DATA    |Mobile internet|\n",
    "|3G    |G network|\n",
    "|AV    |Average|\n",
    "|VOL    |Mobile internet usage volume in MB|\n",
    "|2G    |G network|\n",
    "|PCK    |Prepaid service schemes called  PACKS|\n",
    "|NIGHT    |Scheme to use during specific night hours only|\n",
    "|MONTHLY    |Service schemes with validity equivalent to a month|\n",
    "|SACHET   |Service schemes with validity smaller than a month|\n",
    "|*.6    |KPI for the month of June|\n",
    "|*.7    |KPI for the month of July|\n",
    "|*.8    |KPI for the month of August|\n",
    "|FB_USER|Service scheme to avail services of Facebook and similar social networking sites|\n",
    "|VBC    |Volume based cost  when no specific scheme is not purchased and paid as per usage|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "id": "f5d85c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "id": "1a0e9f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_summary = detailed_summary(data)\n",
    "combined_summary.sort_values(by=['unique_values'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7dec929",
   "metadata": {},
   "source": [
    "## Initial review of data summary\n",
    "1. Column `circle_id` can go as it only has one unique value across all users.\n",
    "2. We have also have columns last_date_of_month_6,  last_date_of_month_7 and last_date_of_month_8 which can go as well as these are just date for end of the month.\n",
    "3. We have columns fb_user_* with binary value and most likley defining user or non user of fb pack. These columns have high missig value of ~74%. Same with columns night_pck_user_*\n",
    "4. Columns 'std_ic_t2o_mou_6', 'std_ic_t2o_mou_7' and 'std_ic_t2o_mou_8' have all zeros and few missing.\n",
    "5. Columns 'loc_og_t2o_mou', 'std_og_t2o_mou' and 'loc_ic_t2o_mou' have same story of all zeroes but no missing.  \n",
    "6. Columns 'std_og_t2c_mou_6', 'std_og_t2c_mou_7' and 'std_og_t2c_mou_8' have all zeros and few missing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6a6297",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcb7c42",
   "metadata": {},
   "source": [
    "## Dropping columns based on initial review\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "id": "c34fa01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "columns_to_drop_initial_review = ['circle_id', \n",
    "                                  'loc_og_t2o_mou', \n",
    "                                  'std_og_t2o_mou', \n",
    "                                  'loc_ic_t2o_mou', \n",
    "                                  'std_ic_t2o_mou_6', 'std_ic_t2o_mou_7', 'std_ic_t2o_mou_8',\n",
    "                                  'std_og_t2c_mou_6', 'std_og_t2c_mou_7', 'std_og_t2c_mou_8',\n",
    "                                  'last_date_of_month_6', 'last_date_of_month_7', 'last_date_of_month_8'\n",
    "] \n",
    "\n",
    "data = data.drop(columns=columns_to_drop_initial_review)\n",
    "unseen = unseen.drop(columns=columns_to_drop_initial_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e850e811",
   "metadata": {},
   "source": [
    "We have removed all columns with unique values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9ae962",
   "metadata": {},
   "source": [
    "## Fix data type for categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "id": "cc12ef4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(data.nunique()).sort_values(by=[0]).head(n=10) # Checking if we have columns with unique values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c68cd96",
   "metadata": {},
   "source": [
    "Let's put fb_user_* and night_pck_user_* into a bucket as these are categorical columns with True and False values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "id": "2f8d5f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_categorical = find_columns_by_regex(data=data, regex_pattern=r'.*fb_user.*|.*night_pck_user.*')\n",
    "binary_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "id": "67489b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[binary_categorical] = data[binary_categorical].astype('category')\n",
    "unseen[binary_categorical] = unseen[binary_categorical].astype('category')\n",
    "data[['churn_probability']] = data[['churn_probability']].astype('category') # Reminding ourselves churn_probability is not included in the unseen data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d82e844",
   "metadata": {},
   "source": [
    "## Date columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "id": "3fedecd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_columns = [v for v in data.columns if v.startswith('date_')]\n",
    "date_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd2d3a1",
   "metadata": {},
   "source": [
    "We have determined that the dates provided represent the last recharge within each month. However, we do not know whether the absence of a recharge in a given month indicates that the customer did not recharge at all or had opted for a longer validity in the previous month.\n",
    "\n",
    "Due to the lack of additional information, we will drop these columns as they cannot be reliably used in our analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "id": "ee773164",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(columns=date_columns)\n",
    "unseen = unseen.drop(columns=date_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56849369",
   "metadata": {},
   "source": [
    "## Get columns with high missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "id": "135cdf86",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_with_70perct_missing = list(data.columns[data.isnull().mean() > 0.50])\n",
    "columns_with_70perct_missing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163bb1e2",
   "metadata": {},
   "source": [
    "We have determined that these columns do not actually have missing data; instead, the absence of data carries meaning. We will handle each set of columns accordingly based on their specific context.\n",
    "\n",
    "- Variables like `total_rech_num_6` can be filled with `0` to indicate zero recharges.\n",
    "- Columns such as `night_pck_user_*` and `fb_user_*` are categorical, and missing data indicates the absence of a subscription to the service. We will mark these missing values as `-1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "id": "7c4adf42",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_zero_fill = find_columns_by_regex(data=data,\n",
    "                                             regex_pattern=r'_rech_|arpu_3g|arpu_2g')\n",
    "columns_to_zero_fill\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823267b8",
   "metadata": {},
   "source": [
    "## Columns with high missing values to fill with zero\n",
    "\n",
    "```python \n",
    "[\n",
    "    'total_rech_num_6',\n",
    "    'total_rech_num_7',\n",
    "    'total_rech_num_8',\n",
    "    'total_rech_amt_6',\n",
    "    'total_rech_amt_7',\n",
    "    'total_rech_amt_8',\n",
    "    'max_rech_amt_6',\n",
    "    'max_rech_amt_7',\n",
    "    'max_rech_amt_8',\n",
    "    'total_rech_data_6',\n",
    "    'total_rech_data_7',\n",
    "    'total_rech_data_8',\n",
    "    'max_rech_data_6',\n",
    "    'max_rech_data_7',\n",
    "    'max_rech_data_8',\n",
    "    'count_rech_2g_6',\n",
    "    'count_rech_2g_7',\n",
    "    'count_rech_2g_8',\n",
    "    'count_rech_3g_6',\n",
    "    'count_rech_3g_7',\n",
    "    'count_rech_3g_8',\n",
    "    'av_rech_amt_data_6',\n",
    "    'av_rech_amt_data_7',\n",
    "    'av_rech_amt_data_8',\n",
    "    'arpu_3g_6',\n",
    "    'arpu_3g_7',\n",
    "    'arpu_3g_8',\n",
    "    'arpu_2g_6',\n",
    "    'arpu_2g_7',\n",
    "    'arpu_2g_8'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "id": "c733525c",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_zero_fill = [\n",
    "    'total_rech_num_6',\n",
    "    'total_rech_num_7',\n",
    "    'total_rech_num_8',\n",
    "    'total_rech_amt_6',\n",
    "    'total_rech_amt_7',\n",
    "    'total_rech_amt_8',\n",
    "    'max_rech_amt_6',\n",
    "    'max_rech_amt_7',\n",
    "    'max_rech_amt_8',\n",
    "    'total_rech_data_6',\n",
    "    'total_rech_data_7',\n",
    "    'total_rech_data_8',\n",
    "    'max_rech_data_6',\n",
    "    'max_rech_data_7',\n",
    "    'max_rech_data_8',\n",
    "    'count_rech_2g_6',\n",
    "    'count_rech_2g_7',\n",
    "    'count_rech_2g_8',\n",
    "    'count_rech_3g_6',\n",
    "    'count_rech_3g_7',\n",
    "    'count_rech_3g_8',\n",
    "    'av_rech_amt_data_6',\n",
    "    'av_rech_amt_data_7',\n",
    "    'av_rech_amt_data_8',\n",
    "    'arpu_3g_6',\n",
    "    'arpu_3g_7',\n",
    "    'arpu_3g_8',\n",
    "    'arpu_2g_6',\n",
    "    'arpu_2g_7',\n",
    "    'arpu_2g_8'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "id": "1d9df67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[columns_to_zero_fill] = data[columns_to_zero_fill].fillna(0)\n",
    "unseen[columns_to_zero_fill] = unseen[columns_to_zero_fill].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f513a84c",
   "metadata": {},
   "source": [
    "## Categorical variables to fill with -1 to peserve meaning of the existing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "id": "daf3a78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add -1 as a new category to the categorical columns\n",
    "data[binary_categorical] = data[binary_categorical].apply(lambda col: col.cat.add_categories([-1]))\n",
    "unseen[binary_categorical] = unseen[binary_categorical].apply(lambda col: col.cat.add_categories([-1]))\n",
    "\n",
    "# Now, fill missing values with -1\n",
    "data[binary_categorical] = data[binary_categorical].fillna(-1)\n",
    "unseen[binary_categorical] = unseen[binary_categorical].fillna(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e7f755",
   "metadata": {},
   "source": [
    "## Columns with low missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "id": "72376126",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_with_few_missing = list(data.columns[(data.isnull().mean() < 0.50) & (data.isnull().mean() > 0)])\n",
    "len(columns_with_few_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "id": "e504cde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[columns_with_few_missing].head(n=5).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "id": "d7b7adc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# detailed_summary(train[columns_with_few_missing])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "id": "bc144f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_imputer, imputed_data = handle_missing_values(\n",
    "    data=data[columns_with_few_missing],\n",
    "    strategy='median',\n",
    "    impute_categorical=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "id": "14457fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNNImputer, imputed_data = handle_missing_values(\n",
    "#     data=train[columns_with_few_missing],\n",
    "#     strategy='knn',\n",
    "#     impute_categorical=True\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "id": "ed939d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "detailed_summary(data[columns_with_few_missing]).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "id": "d868050c",
   "metadata": {},
   "outputs": [],
   "source": [
    "detailed_summary(imputed_data).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "id": "c938aee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[columns_with_few_missing] = imputed_data # Train data updated with imputed data\n",
    "unseen[columns_with_few_missing] = simple_imputer.transform(unseen[columns_with_few_missing] ) # Unseen data transformed with imputer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "id": "d3e84a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.to_pickle('train.pkl')\n",
    "# unseen.to_pickle('unseen.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "id": "ecc93b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(data.columns[(data.isnull().mean() > 0)]) # this list should be empty indicating there are no columns with missing values at this stage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "id": "d261e9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(unseen.columns[(unseen.isnull().mean() > 0)]) # this list should be empty indicating there are no columns with missing values at this stage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fac06be",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30537f18",
   "metadata": {},
   "source": [
    "## Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "id": "9f79f729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are not only exporting correlation scores to CSV but also ensuring each pair only appears once, reducing overload of information. \n",
    "\n",
    "# Create a temporary copy of the data without the 'id' column\n",
    "data_temp = data.drop(columns=['id'])\n",
    "\n",
    "# Divide the data by churn probability\n",
    "data_churn_0 = data_temp[data_temp['churn_probability'] == 0]\n",
    "data_churn_1 = data_temp[data_temp['churn_probability'] == 1]\n",
    "\n",
    "# Calculate the correlation matrix for each group and the overall correlation matrix\n",
    "correlation_matrix_total = data_temp.corr().round(4)\n",
    "correlation_matrix_0 = data_churn_0.corr().round(4)\n",
    "correlation_matrix_1 = data_churn_1.corr().round(4)\n",
    "\n",
    "# Get the upper triangle of the correlation matrix for each group and the total\n",
    "upper_triangle_total = correlation_matrix_total.where(\n",
    "    np.triu(np.ones(correlation_matrix_total.shape), k=1).astype(bool)\n",
    ")\n",
    "upper_triangle_0 = correlation_matrix_0.where(\n",
    "    np.triu(np.ones(correlation_matrix_0.shape), k=1).astype(bool)\n",
    ")\n",
    "upper_triangle_1 = correlation_matrix_1.where(\n",
    "    np.triu(np.ones(correlation_matrix_1.shape), k=1).astype(bool)\n",
    ")\n",
    "\n",
    "# Unstack the matrices and drop NaN values\n",
    "correlation_pairs_total = upper_triangle_total.unstack().dropna()\n",
    "correlation_pairs_0 = upper_triangle_0.unstack().dropna()\n",
    "correlation_pairs_1 = upper_triangle_1.unstack().dropna()\n",
    "\n",
    "# Convert the correlation pairs to dataframes\n",
    "correlation_pairs_df_total = correlation_pairs_total.reset_index()\n",
    "correlation_pairs_df_total.columns = ['Variable 1', 'Variable 2', 'Correlation (Total)']\n",
    "\n",
    "correlation_pairs_df_0 = correlation_pairs_0.reset_index()\n",
    "correlation_pairs_df_0.columns = ['Variable 1', 'Variable 2', 'Correlation (Churn=0)']\n",
    "\n",
    "correlation_pairs_df_1 = correlation_pairs_1.reset_index()\n",
    "correlation_pairs_df_1.columns = ['Variable 1', 'Variable 2', 'Correlation (Churn=1)']\n",
    "\n",
    "# Merge the correlation dataframes\n",
    "correlation_pairs_df = correlation_pairs_df_total.merge(correlation_pairs_df_0, on=['Variable 1', 'Variable 2'], how='left')\n",
    "correlation_pairs_df = correlation_pairs_df.merge(correlation_pairs_df_1, on=['Variable 1', 'Variable 2'], how='left')\n",
    "correlation_pairs_df['Correlation (Total) (ABS)'] = correlation_pairs_df['Correlation (Total)'].abs()\n",
    "correlation_pairs_df['Correlation (Churn=0) (ABS)'] = correlation_pairs_df['Correlation (Churn=0)'].abs()\n",
    "correlation_pairs_df['Correlation (Churn=1) (ABS)'] = correlation_pairs_df['Correlation (Churn=1)'].abs()\n",
    "\n",
    "# Save the merged correlation pairs to a CSV file\n",
    "correlation_pairs_df.to_csv('correlation_pairs_combined.csv', index=False)\n",
    "\n",
    "print(\"Combined correlation pairs saved to 'correlation_pairs_combined.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ccfb84",
   "metadata": {},
   "source": [
    "1. Revenue v/s recharge\n",
    "- Correlation of revenue with respective month indicate recharge for the month is directly reflected towards revenue for the month\n",
    "- Low correlation between revenue and recharge amounged churned users may also indicate users tend to spend less as they churn\n",
    "- Number of recharge follow similat trend\n",
    "\n",
    "| Variable 1 | Variable 2 | Correlation (Total) | Correlation (Churn=0) | Correlation (Churn=1) |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| total_rech_amt_6 | arpu_6 | 0.9528 | 0.9536 | 0.9477 |\n",
    "| total_rech_amt_6 | arpu_7 | 0.7511 | 0.7880 | 0.4823 |\n",
    "| total_rech_amt_6 | arpu_8 | 0.6390 | 0.6936 | 0.2420 |\n",
    "| total_rech_amt_7 | arpu_6 | 0.7260 | 0.7705 | 0.4173 |\n",
    "| total_rech_amt_7 | arpu_7 | 0.9553 | 0.9560 | 0.9461 |\n",
    "| total_rech_amt_7 | arpu_8 | 0.7788 | 0.8003 | 0.5374 |\n",
    "| total_rech_amt_8 | arpu_6 | 0.6268 | 0.6871 | 0.1989 |\n",
    "| total_rech_amt_8 | arpu_7 | 0.7672 | 0.7913 | 0.4927 |\n",
    "| total_rech_amt_8 | arpu_8 | 0.9583 | 0.9582 | 0.9405 |\n",
    "\n",
    "<br />\n",
    "\n",
    "| Variable 1 | Variable 2 | Correlation (Total) | Correlation (Churn=0) | Correlation (Churn=1) |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| total_rech_num_7 | total_rech_num_6 | 0.6703 | 0.7002 | 0.4854 |\n",
    "| total_rech_num_8 | total_rech_num_6 | 0.5167 | 0.5649 | 0.2084 |\n",
    "| total_rech_num_8 | total_rech_num_7 | 0.7239 | 0.7418 | 0.5518 |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413cba81",
   "metadata": {},
   "source": [
    "2. Outgoin ISD calls\n",
    "\n",
    "- Users likey start making less call as the are about to churn as shown in reduced correlation in churn v/s previous month \n",
    "- User which do not churn continue to likely make similar amount of calls\n",
    "\n",
    "| Variable 1 | Variable 2 | Correlation (Total) | Correlation (Churn=0) | Correlation (Churn=1) |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| total_og_mou_8 | total_og_mou_7 | 0.7693 | 0.7928 | 0.5873 |\n",
    "| total_og_mou_7 | total_og_mou_6 | 0.7245 | 0.7464 | 0.6102 |\n",
    "| total_og_mou_8 | total_og_mou_6 | 0.5659 | 0.6157 | 0.2840 |\n",
    "\n",
    "<br />\n",
    "\n",
    "| Variable 1 | Variable 2 | Correlation (Total) | Correlation (Churn=0) | Correlation (Churn=1) |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| isd_og_mou_8 | isd_og_mou_7 | 0.9470 | 0.9582 | 0.5492 |\n",
    "| isd_og_mou_7 | isd_og_mou_6 | 0.9340 | 0.9588 | 0.4444 |\n",
    "| isd_og_mou_8 | isd_og_mou_6 | 0.9241 | 0.9526 | 0.2640 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7372f0",
   "metadata": {},
   "source": [
    "3. Churn v/s age of network\n",
    "- Very low negative correlation indicating as age increases churn probability reduces but is not guaranteed\n",
    "- This correlation is likely affected by class imbalance as well and I would other expect bit higher negative correlation\n",
    "\n",
    "| Variable 1 | Variable 2 | Correlation (Total) |\n",
    "| --- | --- | --- |\n",
    "| churn_probability | aon | -0.1352 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23816ca4",
   "metadata": {},
   "source": [
    "4. Within operator network and outside operator network\n",
    "- Similar trend of low correlation between months which are further apart, indicating call volume goes down as people churn.\n",
    "\n",
    "| Variable 1 | Variable 2 | Correlation (Total) | Correlation (Churn=0) | Correlation (Churn=1) |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| onnet_mou_8 | onnet_mou_7 | 0.7875 | 0.8081 | 0.6340 |\n",
    "| onnet_mou_7 | onnet_mou_6 | 0.7339 | 0.7510 | 0.6528 |\n",
    "| onnet_mou_8 | onnet_mou_6 | 0.5978 | 0.6421 | 0.3567 |\n",
    "\n",
    "<br />\n",
    "\n",
    "| Variable 1 | Variable 2 | Correlation (Total) | Correlation (Churn=0) | Correlation (Churn=1) |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| offnet_mou_8 | offnet_mou_7 | 0.7557 | 0.7796 | 0.5331 |\n",
    "| offnet_mou_7 | offnet_mou_6 | 0.7405 | 0.7597 | 0.6321 |\n",
    "| offnet_mou_8 | offnet_mou_6 | 0.5734 | 0.6178 | 0.2713 |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f3c8db",
   "metadata": {},
   "source": [
    "## Change in monthly KPI churn vs non-churn\n",
    "Let's explore these monthly KPI relationship little bit more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "id": "808d464d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_monthly_column_groups(data: pd.DataFrame) -> list[tuple[str, str, str]]:\n",
    "    \"\"\"\n",
    "    Find columns with names ending in _6, _7, and _8 and return them as a list of tuples.\n",
    "\n",
    "    Parameters:\n",
    "    data (pd.DataFrame): The DataFrame containing the column names.\n",
    "\n",
    "    Returns:\n",
    "    list[tuple[str, str, str]]: A list of tuples, where each tuple contains the column names for June (_6), July (_7), and August (_8).\n",
    "    \"\"\"\n",
    "    columns_6 = [col for col in data.columns if col.endswith('_6')]\n",
    "    columns_7 = [col for col in data.columns if col.endswith('_7')]\n",
    "    columns_8 = [col for col in data.columns if col.endswith('_8')]\n",
    "\n",
    "    monthly_columns = []\n",
    "    for col_6 in columns_6:\n",
    "        base_name = col_6[:-2]\n",
    "        col_7 = f'{base_name}_7'\n",
    "        col_8 = f'{base_name}_8'\n",
    "        if col_7 in columns_7 and col_8 in columns_8:\n",
    "            monthly_columns.append((col_6, col_7, col_8))\n",
    "\n",
    "    return monthly_columns\n",
    "\n",
    "def plot_usage_volume(data: pd.DataFrame, col_june: str, col_july: str, col_aug: str) -> None:\n",
    "    \"\"\"\n",
    "    Plot line graphs showing average June, July, and August volume, split by churn and non-churn.\n",
    "\n",
    "    Parameters:\n",
    "    data (pd.DataFrame): The DataFrame containing user data, including a unique identifier for each respondent and a churn indicator.\n",
    "    col_june (str): The column name for June data.\n",
    "    col_july (str): The column name for July data.\n",
    "    col_aug (str): The column name for August data.\n",
    "    \"\"\"\n",
    "    # Split data into churned and non-churned users\n",
    "    churned = data[data['churn_probability'] == 1]\n",
    "    non_churned = data[data['churn_probability'] == 0]\n",
    "\n",
    "    # Calculate mean volumes for June, July, and August for churned and non-churned users\n",
    "    june_churned_mean = churned[col_june].mean()\n",
    "    july_churned_mean = churned[col_july].mean()\n",
    "    aug_churned_mean = churned[col_aug].mean()\n",
    "\n",
    "    june_non_churned_mean = non_churned[col_june].mean()\n",
    "    july_non_churned_mean = non_churned[col_july].mean()\n",
    "    aug_non_churned_mean = non_churned[col_aug].mean()\n",
    "\n",
    "    # Prepare data for plotting\n",
    "    categories = ['June', 'July', 'August']\n",
    "    churned_values = [june_churned_mean, july_churned_mean, aug_churned_mean]\n",
    "    non_churned_values = [june_non_churned_mean, july_non_churned_mean, aug_non_churned_mean]\n",
    "\n",
    "    base_name = col_june[:-2]\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(categories, churned_values, label='Churned', marker='o', color='red')\n",
    "    plt.plot(categories, non_churned_values, label='Non-Churned', marker='o', color='green')\n",
    "\n",
    "    plt.xlabel('Month')\n",
    "    plt.ylabel('Average Usage Volume')\n",
    "    plt.title(f'{base_name}\\nAverage Usage Volume Comparison: Churned vs Non-Churned Users')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "id": "e9ba64cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns = data.select_dtypes(include=['number']).columns       # We are only interested in volume baed data for the next plots. \n",
    "column_groups = get_monthly_column_groups (data=data[numeric_columns])  # Get monthly KPI as tuples of June, July and August in that order.\n",
    "print (f'Plotting {len(column_groups)} KPI of pairs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "id": "0f59aa99",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in column_groups:\n",
    "    #---------------------------------------------\n",
    "    # Plotting\n",
    "    #--------------- ------------------------------\n",
    "    print (f'{c[0][:-2]}')\n",
    "    plot_usage_volume(\n",
    "        data=data,\n",
    "        col_june=c[0],\n",
    "        col_july=c[1],\n",
    "        col_aug=c[2]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6cbb52",
   "metadata": {},
   "source": [
    "## Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "id": "6a66405d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def outlier_handling(dataframe):\n",
    "    \"\"\"\n",
    "    Applies outlier handling transformations to the values in the given DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - dataframe (DataFrame): The DataFrame containing the values to be transformed.\n",
    "\n",
    "    Returns:\n",
    "    - transformed_values (list): A list containing the transformed values based on the specified conditions.\n",
    "    \"\"\"\n",
    "\n",
    "    transformed_values = []\n",
    "    for value in dataframe:\n",
    "        if value <= 0:\n",
    "            transformed_values.append(float(0))\n",
    "        elif float(value) == float(1.0):\n",
    "            transformed_values.append(float(1.5))\n",
    "        else:\n",
    "            transformed_values.append(float(np.log1p(value)))\n",
    "\n",
    "    return transformed_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "id": "9a3e506a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe(percentiles=[.25, .50, .75, .90, .95, .99]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "id": "cf563c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_columns = list(data.select_dtypes(include=['number']))\n",
    "numerical_columns.remove('id')\n",
    "numerical_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "id": "5d399d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[numerical_columns] = data[numerical_columns].apply(outlier_handling)\n",
    "unseen[numerical_columns] = unseen[numerical_columns].apply(outlier_handling)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "id": "8632b62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape, unseen.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50545613",
   "metadata": {},
   "source": [
    "# High Value customers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c4d144",
   "metadata": {},
   "source": [
    "- We will use total recharge amount to determine high value customer\n",
    "- Arpu has negative values making it difficult candidate for the task\n",
    "- We have total_rech_amt_* to get calling recharge but no equivalent for data recharge\n",
    "- We will use av_rech_amt_data_* and total_rech_data_* to get to total_rech_data_amt_*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "id": "ecb120d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_off = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "id": "ce5c8daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating total data recharge amounts for months 6, 7, and 8\n",
    "total_rech_data_amt = pd.DataFrame({\n",
    "    'total_rech_data_amt_6': data['total_rech_data_6'] * data['av_rech_amt_data_6'],\n",
    "    'total_rech_data_amt_7': data['total_rech_data_7'] * data['av_rech_amt_data_7'],\n",
    "    'total_rech_data_amt_8': data['total_rech_data_8'] * data['av_rech_amt_data_8']\n",
    "})\n",
    "\n",
    "# Concatenating new columns to the existing DataFrame\n",
    "data = pd.concat([data, total_rech_data_amt], axis=1)\n",
    "\n",
    "# Adding total recharge amounts (regular and data recharges) for months 6, 7, and 8\n",
    "data['total_amt_6'] = data['total_rech_amt_6'] + data['total_rech_data_amt_6']\n",
    "data['total_amt_7'] = data['total_rech_amt_7'] + data['total_rech_data_amt_7']\n",
    "data['total_amt_8'] = data['total_rech_amt_8'] + data['total_rech_data_amt_8']\n",
    "\n",
    "# Calculating the 75th percentile for each month's total recharge amount\n",
    "percentiles = data[['total_amt_6', 'total_amt_7', 'total_amt_8']].quantile(cut_off)\n",
    "\n",
    "# Defining high-value customers based on the 75th percentile\n",
    "data['high_value_customer'] = (\n",
    "    (data['total_amt_6'] > percentiles['total_amt_6']) |\n",
    "    (data['total_amt_7'] > percentiles['total_amt_7']) |\n",
    "    (data['total_amt_8'] > percentiles['total_amt_8'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "id": "bbc30d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[['id', 'high_value_customer']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "id": "6de69bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating total data recharge amounts for months 6, 7, and 8\n",
    "total_rech_data_amt = pd.DataFrame({\n",
    "    'total_rech_data_amt_6': unseen['total_rech_data_6'] * unseen['av_rech_amt_data_6'],\n",
    "    'total_rech_data_amt_7': unseen['total_rech_data_7'] * unseen['av_rech_amt_data_7'],\n",
    "    'total_rech_data_amt_8': unseen['total_rech_data_8'] * unseen['av_rech_amt_data_8']\n",
    "})\n",
    "\n",
    "# Concatenating new columns to the existing DataFrame\n",
    "unseen = pd.concat([unseen, total_rech_data_amt], axis=1)\n",
    "\n",
    "# Adding total recharge amounts (regular and data recharges) for months 6, 7, and 8\n",
    "unseen['total_amt_6'] = unseen['total_rech_amt_6'] + unseen['total_rech_data_amt_6']\n",
    "unseen['total_amt_7'] = unseen['total_rech_amt_7'] + unseen['total_rech_data_amt_7']\n",
    "unseen['total_amt_8'] = unseen['total_rech_amt_8'] + unseen['total_rech_data_amt_8']\n",
    "\n",
    "# Calculating the 75th percentile for each month's total recharge amount\n",
    "# Using same percentile as train data\n",
    "\n",
    "# Defining high-value customers based on the 75th percentile\n",
    "unseen['high_value_customer'] = (\n",
    "    (unseen['total_amt_6'] > percentiles['total_amt_6']) |\n",
    "    (unseen['total_amt_7'] > percentiles['total_amt_7']) |\n",
    "    (unseen['total_amt_8'] > percentiles['total_amt_8'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "id": "20511e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "unseen[['id', 'high_value_customer']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "id": "d90f4b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['high_value_customer'].value_counts()/ len(data) *100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "id": "deb14716",
   "metadata": {},
   "outputs": [],
   "source": [
    "unseen['high_value_customer'].value_counts()/ len(unseen) *100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "id": "92c55b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['high_value_customer'] = data['high_value_customer'].astype(int).astype('category')\n",
    "unseen['high_value_customer'] = unseen['high_value_customer'].astype(int).astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "id": "3861680a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_copy = data.copy(deep=True)\n",
    "# data = data[data['high_value_customer'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "id": "3d7940a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape, unseen.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfdf0f7",
   "metadata": {},
   "source": [
    "# Feature Egineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0df81d5",
   "metadata": {},
   "source": [
    "Confused about slope -- why it does improve perfomance of prediction and rather decreased accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4aa1ad",
   "metadata": {},
   "source": [
    "## Slope\n",
    "\n",
    "We have seen decresing trend across months for churn users v/s increasing or stable trend for non-churn users.\n",
    "We will capture this trend as slope for each monthly KPI\n",
    "\n",
    "Though trend is not significant for few KPI, we will calculate slope for all variables and tackle extra variable during feature selection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "id": "4a28e37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_in_usages(data: pd.DataFrame, col_june: str, col_july: str, col_aug: str) -> Tuple[pd.Series, str]:\n",
    "    \"\"\"\n",
    "    Calculate the rolling slope of usage volume between June, July, and August to capture trends.\n",
    "\n",
    "    Parameters:\n",
    "    data (pd.DataFrame): The DataFrame containing the usage data.\n",
    "    col_june (str): The column name for June data.\n",
    "    col_july (str): The column name for July data.\n",
    "    col_aug (str): The column name for August data.\n",
    "\n",
    "    Returns:\n",
    "    pd.Series with slope\n",
    "    \"\"\"\n",
    "    # Create a DataFrame containing the usage columns for June, July, and August\n",
    "    usage_df = data[[col_june, col_july, col_aug]]\n",
    "    \n",
    "    base_name = f'Slope_{col_june[:-2]}'\n",
    "\n",
    "    # Calculate the slope using numpy's polyfit function (degree 1 for linear fit)\n",
    "    slope = usage_df.apply(lambda row: np.polyfit([1, 2, 3], row, 1)[0], axis=1)\n",
    "    # print (base_name, flush=True)\n",
    "    return slope, base_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "id": "17a81ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "temp_data_main = pd.DataFrame() # Empty data to hold new slope variables.\n",
    "columns_to_drop_main = []\n",
    "for c in column_groups:\n",
    "    # Calculate slop\n",
    "    slope, col_name = change_in_usages(\n",
    "        data=data,\n",
    "        col_june=c[0],\n",
    "        col_july=c[1],\n",
    "        col_aug=c[2]\n",
    "    )\n",
    "    temp_data_main[col_name] = slope\n",
    "    columns_to_drop_main.append(c[0])\n",
    "    columns_to_drop_main.append(c[1])\n",
    "    columns_to_drop_main.append(c[2])\n",
    "\n",
    "data = data.drop(columns=columns_to_drop_main)\n",
    "data = pd.concat([data, temp_data_main], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "id": "408bcd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "temp_data_unseen = pd.DataFrame() # Empty dataframe \n",
    "columns_to_drop_unseen = []\n",
    "for c in column_groups:\n",
    "    # Calculate slop for unseen \n",
    "    slope, col_name = change_in_usages(\n",
    "        data=unseen,\n",
    "        col_june=c[0],\n",
    "        col_july=c[1],\n",
    "        col_aug=c[2]\n",
    "    )\n",
    "    temp_data_unseen[col_name] = slope\n",
    "    columns_to_drop_unseen.append(c[0])\n",
    "    columns_to_drop_unseen.append(c[1])\n",
    "    columns_to_drop_unseen.append(c[2])\n",
    "\n",
    "unseen = unseen.drop(columns=columns_to_drop_unseen)\n",
    "unseen = pd.concat([unseen, temp_data_unseen], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "id": "43f9abce",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop_unseen == columns_to_drop_main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "id": "ffbd0682",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape, unseen.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3110be",
   "metadata": {},
   "source": [
    "## One-hot encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "id": "c642cc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = list(data.select_dtypes(include=['category']).columns)\n",
    "categorical_columns.remove('churn_probability') # Already in the right format.\n",
    "categorical_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "id": "00f88a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.get_dummies(data, columns=categorical_columns, drop_first=True, dtype=float)\n",
    "unseen = pd.get_dummies(unseen, columns=categorical_columns, drop_first=True, dtype=float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "id": "780db1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape, unseen.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaedbeb2",
   "metadata": {},
   "source": [
    "# Train - test split and SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "id": "fd2b965d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(columns=['churn_probability'])\n",
    "y = data['churn_probability']\n",
    "\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "id": "83051658",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "X_train.shape, X_val.shape, y_train.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9867ff9a",
   "metadata": {},
   "source": [
    "# Handling data imbalance using SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "id": "5b638b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts() / len (y_train) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a016f2d",
   "metadata": {},
   "source": [
    "We have imbalance in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "id": "06366a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from imblearn.over_sampling import KMeansSMOTE\n",
    "# kmeans_smote = KMeansSMOTE(sampling_strategy=0.5, random_state=42, kmeans_estimator=5, cluster_balance_threshold=0.1)\n",
    "# X_resampled, y_resampled = kmeans_smote.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "id": "61bcffc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "y_resampled.value_counts() / len (y_resampled) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "id": "50f75e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_resampled.shape, y_resampled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "id": "47ca0229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handy function to write submisson file. \n",
    "def create_submission(id : pd.Series, prediction : pd.Series, model_name : str) -> None:\n",
    "    submission = pd.DataFrame({'id': id.astype(int), 'churn_probability': prediction})\n",
    "    submission.to_csv(f'submission_{model_name}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "id": "419630a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "def plot_roc_curve(y_true, y_pred_prob):\n",
    "    \"\"\"\n",
    "    Plots the ROC curve for the given true labels and predicted probabilities.\n",
    "\n",
    "    Parameters:\n",
    "    y_true (array-like): True binary labels (0s and 1s).\n",
    "    y_pred_prob (array-like): Predicted probabilities for the positive class (1).\n",
    "    \n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Calculate the ROC curve and AUC\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_pred_prob)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    # Plotting the ROC curve\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, color='blue', label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='red', linestyle='--')  # Diagonal line\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad53848f",
   "metadata": {},
   "source": [
    "# PCA for Determining Critical Components\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18b0b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the data before applying PCA\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_resampled)  # Assuming X_resampled is the feature data\n",
    "\n",
    "# Apply PCA without specifying n_components to capture all components\n",
    "pca = PCA()\n",
    "pca.fit(X_scaled)\n",
    "\n",
    "# Get the explained variance ratio\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "\n",
    "# Plot the explained variance to visualize how much variance each component explains\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(range(1, len(explained_variance) + 1), explained_variance, marker='o', linestyle='--')\n",
    "plt.title('Explained Variance by Principal Components')\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Calculate the cumulative explained variance\n",
    "cumulative_explained_variance = pca.explained_variance_ratio_.cumsum()\n",
    "\n",
    "# Plot the cumulative explained variance to identify the number of components needed\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(range(1, len(cumulative_explained_variance) + 1), cumulative_explained_variance, marker='o', linestyle='--')\n",
    "plt.title('Cumulative Explained Variance by Principal Components')\n",
    "plt.xlabel('Number of Principal Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Find the number of components that explain at least 90% of the variance\n",
    "n_components_90 = next(i for i, total_var in enumerate(cumulative_explained_variance) if total_var >= 0.90) + 1\n",
    "n_components_95 = next(i for i, total_var in enumerate(cumulative_explained_variance) if total_var >= 0.95) + 1\n",
    "n_components_99 = next(i for i, total_var in enumerate(cumulative_explained_variance) if total_var >= 0.99) + 1\n",
    "print(f'Number of components explaining at least 90% of the variance: {n_components_90}')\n",
    "print(f'Number of components explaining at least 95% of the variance: {n_components_95}')\n",
    "print(f'Number of components explaining at least 99% of the variance: {n_components_99}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12015528",
   "metadata": {},
   "source": [
    "# Model 1 - Logistic Regression with PCA, RFE and Hyperparameter Tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea71b9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Create a pipeline with StandardScaler, PCA, RFE, and LogisticRegression\n",
    "logreg_pca_rfe_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()), \n",
    "    ('pca', PCA(n_components=n_components_95)),\n",
    "    ('rfe', RFE(estimator=LogisticRegression(random_state=42), n_features_to_select=20)),\n",
    "    ('logreg', LogisticRegression(random_state=42))\n",
    "])\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define a parameter grid for Logistic Regression\n",
    "param_grid = {\n",
    "    'logreg__C': [0.001, 0.01, 0.1, 1, 10, 100],  # Regularization strength\n",
    "    'logreg__penalty': ['l1', 'l2'],              # Penalty type\n",
    "    'pca__n_components': [30, 36, 40],            # PCA component tuning\n",
    "    'rfe__n_features_to_select': [10, 15, 20]     # RFE feature selection tuning\n",
    "}\n",
    "\n",
    "# Create the GridSearchCV object\n",
    "grid_search = GridSearchCV(logreg_pca_rfe_pipeline, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "# Fit the model using GridSearchCV\n",
    "grid_search.fit(X_resampled, y_resampled)\n",
    "\n",
    "# Evaluate the best model on the validation set\n",
    "y_val_pred = grid_search.best_estimator_.predict(X_val)\n",
    "print(classification_report(y_val, y_val_pred))\n",
    "\n",
    "# Plot ROC curve for the best model\n",
    "y_val_pred_prob = grid_search.best_estimator_.predict_proba(X_val)\n",
    "plot_roc_curve(y_val, y_val_pred_prob[:, 1])\n",
    "\n",
    "# Make predictions on the unseen data\n",
    "y_unseen_pred = grid_search.best_estimator_.predict(unseen)\n",
    "create_submission(id=unseen['id'], prediction=y_unseen_pred, model_name='logreg_pca_rfe_tuned')\n",
    "\n",
    "# # Train the pipeline on the resampled data\n",
    "# logreg_pca_rfe_pipeline.fit(X_resampled, y_resampled)\n",
    "\n",
    "# # Make predictions on the validation set\n",
    "# y_val_pred = logreg_pca_rfe_pipeline.predict(X_val)\n",
    "\n",
    "# # Evaluate the model\n",
    "# print(classification_report(y_val, y_val_pred))\n",
    "\n",
    "# # Plot ROC curve\n",
    "# y_val_pred_prob = logreg_pca_rfe_pipeline.predict_proba(X_val)\n",
    "# plot_roc_curve(y_val, y_val_pred_prob[:, 1])\n",
    "\n",
    "# # Create Kaggle submission\n",
    "# y_unseen_pred = logreg_pca_rfe_pipeline.predict(X=unseen)\n",
    "# create_submission(id=unseen['id'], prediction=y_unseen_pred, model_name='logreg_pca_rfe_pipeline')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbdd3a9",
   "metadata": {},
   "source": [
    "# Model 2 - Logistic Regression with RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "id": "e32205f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "logreg_rfe_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()), \n",
    "    ('rfe', RFE(estimator=LogisticRegression(random_state=42, max_iter=500), n_features_to_select=10)),\n",
    "    ('logreg', LogisticRegression(random_state=42, max_iter=500))\n",
    "])\n",
    "\n",
    "# Train the pipeline on the resampled data\n",
    "logreg_rfe_pipeline.fit(X_resampled, y_resampled)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_val_pred = logreg_rfe_pipeline.predict(X=X_val)\n",
    "\n",
    "# Evaluate the model\n",
    "print (classification_report(y_val, y_val_pred))\n",
    "\n",
    "# Plot ROC curve\n",
    "y_val_pred_prob = logreg_rfe_pipeline.predict_proba(X_val)\n",
    "plot_roc_curve(y_val, y_val_pred_prob[:, 1])\n",
    "\n",
    "# Create Kaggle submission \n",
    "y_unseen_pred = logreg_rfe_pipeline.predict(X=unseen)\n",
    "create_submission(id=unseen['id'], prediction=y_unseen_pred, model_name='logreg_rfe_pipeline')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb8135c",
   "metadata": {},
   "source": [
    "# Model 3 - Decision Tree with PCA and GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c386a2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Define the pipeline with StandardScaler, PCA, and DecisionTreeClassifier\n",
    "dt_pca_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()), \n",
    "    ('pca', PCA(n_components=n_components_95)),\n",
    "    ('dt', DecisionTreeClassifier(random_state=42))  # Decision Tree classifier\n",
    "])\n",
    "\n",
    "# Define the parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'dt__max_depth': [5, 10, 15, 20],               \n",
    "    'dt__min_samples_leaf': [5, 10, 20, 50, 100],\n",
    "    'dt__criterion': ['gini', 'entropy']            # Criterion for split\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV with DecisionTree and the parameter grid\n",
    "grid_search = GridSearchCV(estimator=dt_pca_pipeline, param_grid=param_grid, cv=5, scoring='precision', n_jobs=-1)\n",
    "\n",
    "# Fit the grid search on the resampled data\n",
    "grid_search.fit(X_resampled, y_resampled)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_val_pred = grid_search.predict(X_val)\n",
    "\n",
    "# Plot ROC curve\n",
    "y_val_pred_prob = grid_search.predict_proba(X_val)\n",
    "plot_roc_curve(y_val, y_val_pred_prob[:, 1])\n",
    "\n",
    "# Create Kaggle submission with predictions for unseen data\n",
    "y_unseen_pred = grid_search.predict(X=unseen)\n",
    "create_submission(id=unseen['id'], prediction=y_unseen_pred, model_name='dt_pca_pipeline')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3d39d3",
   "metadata": {},
   "source": [
    "# Model 4 - Random Forest with PCA and GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "id": "d76864ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Define the pipeline with StandardScaler, PCA, and RandomForestClassifier\n",
    "rf_pca_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()), \n",
    "    ('pca', PCA(n_components=n_components_95)),  \n",
    "    ('rf', RandomForestClassifier(random_state=42)) \n",
    "])\n",
    "\n",
    "# Define the parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'rf__n_estimators': [100, 200, 300],        # Number of trees\n",
    "    'rf__max_depth': [10, 20, 30],              # Max depth of the trees\n",
    "    'rf__min_samples_split': [2, 5, 10],        # Minimum number of samples required to split\n",
    "    'rf__min_samples_leaf': [1, 2, 4],          # Minimum number of samples at leaf node\n",
    "    'rf__criterion': ['gini', 'entropy']        # Criterion for split\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV with RandomForestClassifier and the parameter grid\n",
    "grid_search_rf = GridSearchCV(estimator=rf_pca_pipeline, param_grid=param_grid, cv=5, scoring='precision', n_jobs=-1)\n",
    "\n",
    "# Fit the grid search on the resampled data\n",
    "grid_search_rf.fit(X_resampled, y_resampled)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_val_pred = grid_search_rf.predict(X_val)\n",
    "\n",
    "# Plot ROC curve\n",
    "y_val_pred_prob = grid_search_rf.predict_proba(X_val)\n",
    "plot_roc_curve(y_val, y_val_pred_prob[:, 1])\n",
    "\n",
    "# Create Kaggle submission with predictions for unseen data\n",
    "y_unseen_pred = grid_search_rf.predict(X=unseen)\n",
    "create_submission(id=unseen['id'], prediction=y_unseen_pred, model_name='rf_pca_pipeline')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaed7072",
   "metadata": {},
   "source": [
    "# Model 5 - XGBoost with PCA and GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29da2bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Define the pipeline with StandardScaler, PCA, and XGBClassifier\n",
    "xgb_pca_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()), \n",
    "    ('pca', PCA(n_components=n_components_95)),\n",
    "    ('xgb', XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss'))  # XGBoost classifier\n",
    "])\n",
    "\n",
    "# Define the parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'xgb__n_estimators': [100, 200, 300],          # Number of boosting rounds\n",
    "    'xgb__max_depth': [3, 5, 7],                   # Maximum depth of a tree\n",
    "    'xgb__learning_rate': [0.01, 0.1, 0.2],        # Step size shrinkage\n",
    "    'xgb__subsample': [0.7, 0.8, 1.0],             # Subsample ratio of the training instances\n",
    "    'xgb__colsample_bytree': [0.7, 0.8, 1.0],      # Subsample ratio of columns when constructing each tree\n",
    "    'xgb__min_child_weight': [1, 3, 5]             # Minimum sum of instance weight needed in a child\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV with XGBClassifier and the parameter grid\n",
    "grid_search_xgb = GridSearchCV(estimator=xgb_pca_pipeline, param_grid=param_grid, cv=5, scoring='precision', n_jobs=-1)\n",
    "\n",
    "# Fit the grid search on the resampled data\n",
    "grid_search_xgb.fit(X_resampled, y_resampled)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_val_pred = grid_search_xgb.predict(X_val)\n",
    "\n",
    "# Evaluate the model using classification report and precision score\n",
    "print(classification_report(y_val, y_val_pred))\n",
    "\n",
    "# Plot ROC curve\n",
    "y_val_pred_prob = grid_search_xgb.predict_proba(X_val)\n",
    "plot_roc_curve(y_val, y_val_pred_prob[:, 1])\n",
    "\n",
    "# Create Kaggle submission with predictions for unseen data\n",
    "y_unseen_pred = grid_search_xgb.predict(X=unseen)\n",
    "create_submission(id=unseen['id'], prediction=y_unseen_pred, model_name='xgb_pca_pipeline')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
